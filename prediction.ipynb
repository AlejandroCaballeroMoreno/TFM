{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('Python39')"
  },
  "metadata": {
   "interpreter": {
    "hash": "1eea1da590da135dc8247d2570e64ce8ef483f82cae9666e2503c03c7d9d6656"
   }
  },
  "interpreter": {
   "hash": "1eea1da590da135dc8247d2570e64ce8ef483f82cae9666e2503c03c7d9d6656"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from yahoo_fin import stock_info as si\n",
    "from collections import deque\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Declarar la semilla, para poder obtener los mismos resultados después de ejecutar el programa varias veces\n",
    "np.random.seed(314)\n",
    "tf.random.set_seed(314)\n",
    "random.seed(314)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARAR EL CONJUNTO DE DATOS\n",
    "def shuffle_in_unison(a,b):\n",
    "    # Mezclar dos vectores del mismo modo\n",
    "    state = np.random.get_state()\n",
    "    np.random.shuffle(a)\n",
    "    np.random.set_state(state)\n",
    "    np.random.shuffle(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(ticker, n_steps = 50, scale = True, shuffle = True, lookup_step = 1, split_by_date = True, test_size = 0.2, feature_columns = ['adjclose','volume','open','high','low']):\n",
    "    \"\"\"\n",
    "    Cargar datos de Yahoo Finance, así como escalar, mezclar, normalizar y dividir.\n",
    "    Parámetros:\n",
    "        ticker (str/pd.DataFrame): La cotización que se quiera cargar, ejemplos incluyen AAPL, TESL, etc. \n",
    "        n_steps (int): la longitud del historico de datos (ejemplo: tamaño de ventana) usado para predecir. Por defecto se usa 50\n",
    "        scale (bool): si las escalas de los precios están entre 0 y 1, por defecto está a True\n",
    "        shuffle (bool): si se mezcla el conjunto de datos (ambos el de entrenamiento y el de test), por defecto esta a True. \n",
    "        lookup_step (int): el próximo paso de búsqueda para realizar la predicción, el valor por defecto es 1 (sería para el siguiente dia)\n",
    "        split_by_date (bool): si separamos el conjunto de datos en entrenamiento/test por fecha, definiéndolo como Falso los conjuntos se separaran de forma aleatoria\n",
    "        test_size (float): ratio para los datos de test, por defecto se usará 0.2 (20% serán datos de test)\n",
    "        feature_columns (list): la lista de caracteristicas que se usará como input del modelo. Como predeterminado, se usarán todos los datos extraídos de Yahoo Finance\n",
    "    \"\"\"\n",
    "\n",
    "    # Se comprueba si la cotización ya se ha extraído previamente de Yahoo Finance\n",
    "    if isinstance(ticker, str):\n",
    "        # Lo extraemos de Yahoo Finance\n",
    "        df = si.get_data(ticker)\n",
    "    elif isinstance(ticker, pd.DataFrame):\n",
    "        # Ya se ha extraído, utilizar directamente\n",
    "        df = ticker\n",
    "    else:\n",
    "        raise TypeError(\"La variable ticker puede ser tanto u string como un 'pd.DataFrame'\")\n",
    "    \n",
    "    # La siguiente variable contendrá todos los elementos que queremos devolver de esta funcion\n",
    "    result = {}\n",
    "    # Devolveremos también el DataFrame original\n",
    "    result['df'] = df.copy()\n",
    "    # Nos debemos asegurar que la variable feature_columns exista dentro del dataframe\n",
    "    for col in feature_columns:\n",
    "        assert col in df.columns, f\"'{col}' no existe en el dataframe.\"\n",
    "    # Añadir en una nueva columna la fecha\n",
    "    if \"date\" not in df.columns:\n",
    "        df[\"date\"] = df.index\n",
    "    if scale:\n",
    "        column_scaler = {}\n",
    "        # Escalar los datos (los precios) entre 0 y 1 \n",
    "        for column in feature_columns:\n",
    "            scaler = preprocessing.MinMaxScaler()\n",
    "            df[column] = scaler.fit_transform(np.expand_dims(df[column].values, axis = 1))\n",
    "            column_scaler[column] = scaler\n",
    "        # Añadir los casos de MinMaxScaler a la variable de salida\n",
    "        result[\"column_scaler\"] = column_scaler\n",
    "    # Añadir la columna \"objetivo\" (etiquetas) desplazando un número = a lookup_step\n",
    "    df['future'] = df['adjclose'].shift(-lookup_step)\n",
    "    # La última columna de lookup_steps contendrá los valores NaN en la columna de future. Si se quisieran predecir 15 stocks, habría 15 columnas NaN\n",
    "    # Almacenamos las columnas antes de desecharlas\n",
    "    last_sequence = np.array(df[feature_columns].tail(lookup_step))\n",
    "    # Desechamos los NaNs\n",
    "    df.dropna(inplace=True)\n",
    "    sequence_data = []\n",
    "    sequences = deque(maxlen=n_steps)\n",
    "    for entry, target in zip(df[feature_columns + [\"date\"]].values, df['future'].values):\n",
    "        sequences.append(entry)\n",
    "        if len(sequences) == n_steps:\n",
    "            sequence_data.append([np.array(sequences), target])\n",
    "    # Obtener la última secuencia (last_sequence) indexando los últimos n_step junto con los lookup_step\n",
    "    # Por ejemplo: si n_steps= 50 y lookup_step = 10, last_sequence debería ser 60\n",
    "    # Esta última secuencia será utilizada para realizar la predicción de los precios de futuros stocks que no están disponibles en los datos que tenemos actualmente\n",
    "    last_sequence = list([s[:len(feature_columns)] for s in sequences]) + list(last_sequence)\n",
    "    last_sequence = np.array(last_sequence).astype(np.float32)\n",
    "    # Lo añadimos a la variable resultado\n",
    "    result['last_sequence'] = last_sequence\n",
    "    # Construimos las X y las y\n",
    "    X, y = [], []\n",
    "    for seq, target in sequence_data:\n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "    # Lo convertimos a vectores de tipo numpy\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    if split_by_date:\n",
    "        # Separamos el conjunto de datos en grupo de entrenamiento y de test según su fecha (la otra forma es separarlos de forma aleatoria)\n",
    "        train_samples = int((1 - test_size)*len(X))\n",
    "        result[\"X_train\"] = X[:train_samples]\n",
    "        result[\"y_train\"] = y[:train_samples]\n",
    "        result[\"X_test\"] = X[train_samples:]\n",
    "        result[\"y_test\"] = y[train_samples:]\n",
    "        if shuffle:\n",
    "            # Mezclamos los conjuntos de datos para su entrenamiento (si shuffle esta activado)\n",
    "            shuffle_in_unison(result[\"X_train\"], result[\"y_train\"])\n",
    "            shuffle_in_unison(result[\"X_test\"], result[\"y_test\"])\n",
    "    else:\n",
    "        # Separamos el conjunto de datos de forma aleatoria\n",
    "        result[\"X_train\"], result[\"X_test\"], result[\"y_train\"], result[\"y_test\"] = train_test_split(X, y, test_size = test_size, shuffle = shuffle)\n",
    "    # Obtenemos una lista con las fechas de los datos dentro del grupo de test\n",
    "    dates = result[\"X_test\"][:, -1, -1]\n",
    "    # Recuperar las características de test del dataframe original\n",
    "    result[\"test_df\"] = result[\"df\"].loc[dates]\n",
    "    # Borrar fechas duplicadas en el dataframe de test\n",
    "    result[\"test_df\"] = result[\"test_df\"][~result[\"test_df\"].index.duplicated(keep='first')]\n",
    "    # Borrar fechas de los conjuntos de entrenamiento y de test y los convertimos a float32\n",
    "    result[\"X_train\"] = result[\"X_train\"][:, :, :len(feature_columns)].astype(np.float32)\n",
    "    result[\"X_test\"] = result[\"X_test\"][:, :, :len(feature_columns)].astype(np.float32)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREACION DEL MODELO\n",
    "def create_model(sequence_length, n_features, units = 256, cell = LSTM, n_layers = 2, dropout = 0.3, loss = \"mean_absolute_error\", optimizer = \"rmsprop\", bidirectional = False):\n",
    "    model = Sequential()\n",
    "    for i in range(n_layers):\n",
    "        if i == 0:\n",
    "            # Primera capa\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences = True), batch_input_shape = (None, sequence_length, n_features)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences = True, batch_input_shape = (None, sequence_length, n_features)))\n",
    "        elif i == n_layers - 1:\n",
    "            # Ultima capa\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences = False)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences = False))\n",
    "        else: \n",
    "            # Capas escondidas\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences = True)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences = True))\n",
    "        # Añadir fallo después de cada capa\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "    model.compile(loss=loss, metrics=[\"mean_absolute_error\"], optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTRENAMIENTO DEL MODELO\n",
    "# Declaramos el tamaño de la secuencia (tamaño de la ventana)\n",
    "\n",
    "# Window size or the sequence length\n",
    "N_STEPS = 50\n",
    "# Lookup step, 1 is the next day\n",
    "LOOKUP_STEP = 15\n",
    "# whether to scale feature columns & output price as well\n",
    "SCALE = True\n",
    "scale_str = f\"sc-{int(SCALE)}\"\n",
    "# whether to shuffle the dataset\n",
    "SHUFFLE = True\n",
    "shuffle_str = f\"sh-{int(SHUFFLE)}\"\n",
    "# whether to split the training/testing set by date\n",
    "SPLIT_BY_DATE = False\n",
    "split_by_date_str = f\"sbd-{int(SPLIT_BY_DATE)}\"\n",
    "# test ratio size, 0.2 is 20%\n",
    "TEST_SIZE = 0.2\n",
    "# features to use\n",
    "FEATURE_COLUMNS = [\"adjclose\", \"volume\", \"open\", \"high\", \"low\"]\n",
    "# date now\n",
    "date_now = time.strftime(\"%Y-%m-%d\")\n",
    "### model parameters\n",
    "N_LAYERS = 2\n",
    "# LSTM cell\n",
    "CELL = LSTM\n",
    "# 256 LSTM neurons\n",
    "UNITS = 256\n",
    "# 40% dropout\n",
    "DROPOUT = 0.4\n",
    "# whether to use bidirectional RNNs\n",
    "BIDIRECTIONAL = False\n",
    "### training parameters\n",
    "# mean absolute error loss\n",
    "# LOSS = \"mae\"\n",
    "# huber loss\n",
    "LOSS = \"huber_loss\"\n",
    "OPTIMIZER = \"adam\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 350\n",
    "# Amazon stock market\n",
    "ticker = \"AMZN\"\n",
    "ticker_data_filename = os.path.join(\"data\", f\"{ticker}_{date_now}.csv\")\n",
    "# model name to save, making it as unique as possible based on parameters\n",
    "model_name = f\"{date_now}_{ticker}-{shuffle_str}-{scale_str}-{split_by_date_str}-\\\n",
    "{LOSS}-{OPTIMIZER}-{CELL.__name__}-seq-{N_STEPS}-step-{LOOKUP_STEP}-layers-{N_LAYERS}-units-{UNITS}\"\n",
    "if BIDIRECTIONAL:\n",
    "    model_name += \"-b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nos aseguramos de que los ficheros de resultados, logs y datos existen antes de entrenar el modelo\n",
    "# Creamos los ficheros si no existen\n",
    "# create these folders if they does not exist\n",
    "if not os.path.isdir(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "if not os.path.isdir(\"logs\"):\n",
    "    os.mkdir(\"logs\")\n",
    "if not os.path.isdir(\"data\"):\n",
    "    os.mkdir(\"data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/350\n",
      "75/75 [==============================] - 27s 321ms/step - loss: 0.0016 - mean_absolute_error: 0.0248 - val_loss: 2.0402e-04 - val_mean_absolute_error: 0.0108\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00020, saving model to results\\2021-06-21_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "Epoch 2/350\n",
      "75/75 [==============================] - 19s 258ms/step - loss: 4.2823e-04 - mean_absolute_error: 0.0139 - val_loss: 2.7973e-04 - val_mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00020\n",
      "Epoch 3/350\n",
      "75/75 [==============================] - 19s 248ms/step - loss: 4.8938e-04 - mean_absolute_error: 0.0158 - val_loss: 2.0990e-04 - val_mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00020\n",
      "Epoch 4/350\n",
      "75/75 [==============================] - 19s 249ms/step - loss: 4.2862e-04 - mean_absolute_error: 0.0141 - val_loss: 1.8216e-04 - val_mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00020 to 0.00018, saving model to results\\2021-06-21_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "Epoch 5/350\n",
      "75/75 [==============================] - 18s 239ms/step - loss: 4.0374e-04 - mean_absolute_error: 0.0138 - val_loss: 2.4167e-04 - val_mean_absolute_error: 0.0107\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00018\n",
      "Epoch 6/350\n",
      "75/75 [==============================] - 18s 244ms/step - loss: 3.7958e-04 - mean_absolute_error: 0.0132 - val_loss: 2.5131e-04 - val_mean_absolute_error: 0.0097\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00018\n",
      "Epoch 7/350\n",
      "75/75 [==============================] - 18s 242ms/step - loss: 3.9557e-04 - mean_absolute_error: 0.0135 - val_loss: 2.1098e-04 - val_mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00018\n",
      "Epoch 8/350\n",
      "75/75 [==============================] - 17s 229ms/step - loss: 4.0661e-04 - mean_absolute_error: 0.0143 - val_loss: 2.5514e-04 - val_mean_absolute_error: 0.0135\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00018\n",
      "Epoch 9/350\n",
      "75/75 [==============================] - 17s 228ms/step - loss: 3.8369e-04 - mean_absolute_error: 0.0135 - val_loss: 2.9249e-04 - val_mean_absolute_error: 0.0115\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00018\n",
      "Epoch 10/350\n",
      "75/75 [==============================] - 17s 228ms/step - loss: 3.6278e-04 - mean_absolute_error: 0.0130 - val_loss: 1.8583e-04 - val_mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00018\n",
      "Epoch 11/350\n",
      "75/75 [==============================] - 17s 229ms/step - loss: 3.5146e-04 - mean_absolute_error: 0.0134 - val_loss: 2.0636e-04 - val_mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00018\n",
      "Epoch 12/350\n",
      "75/75 [==============================] - 17s 225ms/step - loss: 3.6135e-04 - mean_absolute_error: 0.0129 - val_loss: 1.8711e-04 - val_mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00018\n",
      "Epoch 13/350\n",
      "75/75 [==============================] - 17s 227ms/step - loss: 6.8752e-04 - mean_absolute_error: 0.0189 - val_loss: 7.9763e-04 - val_mean_absolute_error: 0.0229\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00018\n",
      "Epoch 14/350\n",
      "75/75 [==============================] - 17s 225ms/step - loss: 4.3771e-04 - mean_absolute_error: 0.0145 - val_loss: 3.5637e-04 - val_mean_absolute_error: 0.0128\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00018\n",
      "Epoch 15/350\n",
      "75/75 [==============================] - 17s 228ms/step - loss: 3.9420e-04 - mean_absolute_error: 0.0137 - val_loss: 2.5370e-04 - val_mean_absolute_error: 0.0097\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00018\n",
      "Epoch 16/350\n",
      "75/75 [==============================] - 17s 229ms/step - loss: 3.9372e-04 - mean_absolute_error: 0.0138 - val_loss: 1.8614e-04 - val_mean_absolute_error: 0.0097\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00018\n",
      "Epoch 17/350\n",
      "75/75 [==============================] - 17s 227ms/step - loss: 3.5755e-04 - mean_absolute_error: 0.0134 - val_loss: 2.4692e-04 - val_mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00018\n",
      "Epoch 18/350\n",
      "75/75 [==============================] - 17s 225ms/step - loss: 3.6974e-04 - mean_absolute_error: 0.0138 - val_loss: 4.2610e-04 - val_mean_absolute_error: 0.0133\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00018\n",
      "Epoch 19/350\n",
      "75/75 [==============================] - 17s 225ms/step - loss: 3.8087e-04 - mean_absolute_error: 0.0139 - val_loss: 3.0159e-04 - val_mean_absolute_error: 0.0123\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00018\n",
      "Epoch 20/350\n",
      "75/75 [==============================] - 17s 225ms/step - loss: 3.7628e-04 - mean_absolute_error: 0.0136 - val_loss: 1.7972e-04 - val_mean_absolute_error: 0.0094\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00018 to 0.00018, saving model to results\\2021-06-21_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "Epoch 21/350\n",
      "75/75 [==============================] - 17s 229ms/step - loss: 3.4794e-04 - mean_absolute_error: 0.0129 - val_loss: 2.1473e-04 - val_mean_absolute_error: 0.0094\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00018\n",
      "Epoch 22/350\n",
      "75/75 [==============================] - 17s 223ms/step - loss: 3.0059e-04 - mean_absolute_error: 0.0125 - val_loss: 1.7040e-04 - val_mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00018 to 0.00017, saving model to results\\2021-06-21_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "Epoch 23/350\n",
      "75/75 [==============================] - 17s 227ms/step - loss: 3.4130e-04 - mean_absolute_error: 0.0131 - val_loss: 2.4640e-04 - val_mean_absolute_error: 0.0099\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00017\n",
      "Epoch 24/350\n",
      "75/75 [==============================] - 17s 221ms/step - loss: 3.1766e-04 - mean_absolute_error: 0.0129 - val_loss: 2.4362e-04 - val_mean_absolute_error: 0.0109\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00017\n",
      "Epoch 25/350\n",
      "75/75 [==============================] - 17s 226ms/step - loss: 3.5339e-04 - mean_absolute_error: 0.0135 - val_loss: 5.0235e-04 - val_mean_absolute_error: 0.0138\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00017\n",
      "Epoch 26/350\n",
      "75/75 [==============================] - 17s 222ms/step - loss: 3.4625e-04 - mean_absolute_error: 0.0134 - val_loss: 1.7328e-04 - val_mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00017\n",
      "Epoch 27/350\n",
      "75/75 [==============================] - 17s 225ms/step - loss: 3.9523e-04 - mean_absolute_error: 0.0141 - val_loss: 5.6021e-04 - val_mean_absolute_error: 0.0168\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00017\n",
      "Epoch 28/350\n",
      "75/75 [==============================] - 17s 223ms/step - loss: 4.1780e-04 - mean_absolute_error: 0.0149 - val_loss: 2.3287e-04 - val_mean_absolute_error: 0.0103\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00017\n",
      "Epoch 29/350\n",
      "75/75 [==============================] - 17s 224ms/step - loss: 3.4334e-04 - mean_absolute_error: 0.0135 - val_loss: 2.3118e-04 - val_mean_absolute_error: 0.0100\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00017\n",
      "Epoch 30/350\n",
      "75/75 [==============================] - 17s 221ms/step - loss: 3.0406e-04 - mean_absolute_error: 0.0129 - val_loss: 4.1579e-04 - val_mean_absolute_error: 0.0140\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00017\n",
      "Epoch 31/350\n",
      "75/75 [==============================] - 17s 221ms/step - loss: 3.0800e-04 - mean_absolute_error: 0.0129 - val_loss: 1.6996e-04 - val_mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00017 to 0.00017, saving model to results\\2021-06-21_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "Epoch 32/350\n",
      "75/75 [==============================] - 17s 223ms/step - loss: 3.2985e-04 - mean_absolute_error: 0.0132 - val_loss: 2.1829e-04 - val_mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00017\n",
      "Epoch 33/350\n",
      "75/75 [==============================] - 17s 230ms/step - loss: 3.7614e-04 - mean_absolute_error: 0.0141 - val_loss: 3.3455e-04 - val_mean_absolute_error: 0.0161\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00017\n",
      "Epoch 34/350\n",
      "75/75 [==============================] - 17s 225ms/step - loss: 3.0812e-04 - mean_absolute_error: 0.0134 - val_loss: 2.9773e-04 - val_mean_absolute_error: 0.0108\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00017\n",
      "Epoch 35/350\n",
      "75/75 [==============================] - 17s 224ms/step - loss: 3.1437e-04 - mean_absolute_error: 0.0134 - val_loss: 1.7419e-04 - val_mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00017\n",
      "Epoch 36/350\n",
      "75/75 [==============================] - 17s 224ms/step - loss: 3.4204e-04 - mean_absolute_error: 0.0138 - val_loss: 2.8295e-04 - val_mean_absolute_error: 0.0109\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00017\n",
      "Epoch 37/350\n",
      "75/75 [==============================] - 17s 229ms/step - loss: 4.1414e-04 - mean_absolute_error: 0.0151 - val_loss: 3.8220e-04 - val_mean_absolute_error: 0.0145\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00017\n",
      "Epoch 38/350\n",
      "75/75 [==============================] - 17s 225ms/step - loss: 3.4475e-04 - mean_absolute_error: 0.0139 - val_loss: 2.0198e-04 - val_mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00017\n",
      "Epoch 39/350\n",
      "75/75 [==============================] - 17s 225ms/step - loss: 3.0979e-04 - mean_absolute_error: 0.0134 - val_loss: 1.9435e-04 - val_mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00017\n",
      "Epoch 40/350\n",
      "75/75 [==============================] - 17s 230ms/step - loss: 4.0547e-04 - mean_absolute_error: 0.0150 - val_loss: 1.7509e-04 - val_mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00017\n",
      "Epoch 41/350\n",
      "75/75 [==============================] - 17s 225ms/step - loss: 3.1002e-04 - mean_absolute_error: 0.0136 - val_loss: 2.4017e-04 - val_mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00017\n",
      "Epoch 42/350\n",
      "75/75 [==============================] - 17s 223ms/step - loss: 2.8790e-04 - mean_absolute_error: 0.0131 - val_loss: 1.6835e-04 - val_mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00017 to 0.00017, saving model to results\\2021-06-21_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "Epoch 43/350\n",
      "75/75 [==============================] - 17s 226ms/step - loss: 3.0769e-04 - mean_absolute_error: 0.0134 - val_loss: 1.7842e-04 - val_mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00017\n",
      "Epoch 44/350\n",
      "75/75 [==============================] - 17s 224ms/step - loss: 3.1274e-04 - mean_absolute_error: 0.0135 - val_loss: 2.2681e-04 - val_mean_absolute_error: 0.0105\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00017\n",
      "Epoch 45/350\n",
      "75/75 [==============================] - 17s 224ms/step - loss: 2.8591e-04 - mean_absolute_error: 0.0130 - val_loss: 1.6759e-04 - val_mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00017 to 0.00017, saving model to results\\2021-06-21_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "Epoch 46/350\n",
      "75/75 [==============================] - 17s 223ms/step - loss: 2.9780e-04 - mean_absolute_error: 0.0135 - val_loss: 1.8387e-04 - val_mean_absolute_error: 0.0100\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00017\n",
      "Epoch 47/350\n",
      "75/75 [==============================] - 17s 222ms/step - loss: 3.2388e-04 - mean_absolute_error: 0.0139 - val_loss: 1.8580e-04 - val_mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00017\n",
      "Epoch 48/350\n",
      "75/75 [==============================] - 17s 222ms/step - loss: 3.1501e-04 - mean_absolute_error: 0.0138 - val_loss: 2.1294e-04 - val_mean_absolute_error: 0.0101\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00017\n",
      "Epoch 49/350\n",
      "75/75 [==============================] - 17s 225ms/step - loss: 3.0042e-04 - mean_absolute_error: 0.0133 - val_loss: 2.4928e-04 - val_mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00017\n",
      "Epoch 50/350\n",
      "75/75 [==============================] - 17s 224ms/step - loss: 3.1118e-04 - mean_absolute_error: 0.0136 - val_loss: 1.6686e-04 - val_mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00017 to 0.00017, saving model to results\\2021-06-21_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "Epoch 51/350\n",
      "75/75 [==============================] - 17s 225ms/step - loss: 2.8583e-04 - mean_absolute_error: 0.0132 - val_loss: 1.9167e-04 - val_mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00017\n",
      "Epoch 52/350\n",
      "75/75 [==============================] - 17s 223ms/step - loss: 3.2907e-04 - mean_absolute_error: 0.0137 - val_loss: 1.8559e-04 - val_mean_absolute_error: 0.0097\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00017\n",
      "Epoch 53/350\n",
      "75/75 [==============================] - 17s 223ms/step - loss: 2.9232e-04 - mean_absolute_error: 0.0135 - val_loss: 1.7238e-04 - val_mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00017\n",
      "Epoch 54/350\n",
      "75/75 [==============================] - 17s 222ms/step - loss: 2.7617e-04 - mean_absolute_error: 0.0131 - val_loss: 1.6049e-04 - val_mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00017 to 0.00016, saving model to results\\2021-06-21_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "Epoch 55/350\n",
      "75/75 [==============================] - 17s 222ms/step - loss: 2.9950e-04 - mean_absolute_error: 0.0136 - val_loss: 1.7885e-04 - val_mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00016\n",
      "Epoch 56/350\n",
      "75/75 [==============================] - 17s 226ms/step - loss: 2.9755e-04 - mean_absolute_error: 0.0135 - val_loss: 2.2357e-04 - val_mean_absolute_error: 0.0112\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00016\n",
      "Epoch 57/350\n",
      "75/75 [==============================] - 17s 226ms/step - loss: 2.9328e-04 - mean_absolute_error: 0.0136 - val_loss: 2.0258e-04 - val_mean_absolute_error: 0.0107\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00016\n",
      "Epoch 58/350\n",
      "75/75 [==============================] - 17s 226ms/step - loss: 3.2458e-04 - mean_absolute_error: 0.0142 - val_loss: 2.0095e-04 - val_mean_absolute_error: 0.0097\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00016\n",
      "Epoch 59/350\n",
      "75/75 [==============================] - 17s 224ms/step - loss: 3.0388e-04 - mean_absolute_error: 0.0135 - val_loss: 1.3894e-04 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00016 to 0.00014, saving model to results\\2021-06-21_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "Epoch 60/350\n",
      "75/75 [==============================] - 17s 225ms/step - loss: 2.6142e-04 - mean_absolute_error: 0.0132 - val_loss: 1.7152e-04 - val_mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00014\n",
      "Epoch 61/350\n",
      "75/75 [==============================] - 17s 225ms/step - loss: 2.9830e-04 - mean_absolute_error: 0.0135 - val_loss: 1.6346e-04 - val_mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00014\n",
      "Epoch 62/350\n",
      "75/75 [==============================] - 17s 224ms/step - loss: 2.8244e-04 - mean_absolute_error: 0.0134 - val_loss: 2.1947e-04 - val_mean_absolute_error: 0.0111\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00014\n",
      "Epoch 63/350\n",
      "75/75 [==============================] - 18s 236ms/step - loss: 2.5426e-04 - mean_absolute_error: 0.0127 - val_loss: 1.5508e-04 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00014\n",
      "Epoch 64/350\n",
      "75/75 [==============================] - 17s 228ms/step - loss: 2.5842e-04 - mean_absolute_error: 0.0129 - val_loss: 1.3407e-04 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00014 to 0.00013, saving model to results\\2021-06-21_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "Epoch 65/350\n",
      "75/75 [==============================] - 17s 225ms/step - loss: 2.7866e-04 - mean_absolute_error: 0.0132 - val_loss: 1.4028e-04 - val_mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00013\n",
      "Epoch 66/350\n",
      "75/75 [==============================] - 17s 225ms/step - loss: 2.6903e-04 - mean_absolute_error: 0.0130 - val_loss: 1.3919e-04 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00013\n",
      "Epoch 67/350\n",
      "75/75 [==============================] - 17s 223ms/step - loss: 2.7050e-04 - mean_absolute_error: 0.0132 - val_loss: 1.6164e-04 - val_mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00013\n",
      "Epoch 68/350\n",
      "75/75 [==============================] - 17s 225ms/step - loss: 2.5796e-04 - mean_absolute_error: 0.0129 - val_loss: 1.3227e-04 - val_mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00013 to 0.00013, saving model to results\\2021-06-21_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "Epoch 69/350\n",
      "75/75 [==============================] - 17s 224ms/step - loss: 2.5964e-04 - mean_absolute_error: 0.0129 - val_loss: 2.0452e-04 - val_mean_absolute_error: 0.0099\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00013\n",
      "Epoch 70/350\n",
      "75/75 [==============================] - 17s 224ms/step - loss: 2.4669e-04 - mean_absolute_error: 0.0127 - val_loss: 1.3525e-04 - val_mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00013\n",
      "Epoch 71/350\n",
      "75/75 [==============================] - 17s 227ms/step - loss: 2.5051e-04 - mean_absolute_error: 0.0126 - val_loss: 1.4184e-04 - val_mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00013\n",
      "Epoch 72/350\n",
      "75/75 [==============================] - 17s 225ms/step - loss: 2.9577e-04 - mean_absolute_error: 0.0134 - val_loss: 1.2910e-04 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00013 to 0.00013, saving model to results\\2021-06-21_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "Epoch 73/350\n",
      "75/75 [==============================] - 17s 223ms/step - loss: 2.5608e-04 - mean_absolute_error: 0.0129 - val_loss: 1.4570e-04 - val_mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00013\n",
      "Epoch 74/350\n",
      "75/75 [==============================] - 17s 223ms/step - loss: 2.7733e-04 - mean_absolute_error: 0.0134 - val_loss: 1.4297e-04 - val_mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00013\n",
      "Epoch 75/350\n",
      "75/75 [==============================] - 17s 224ms/step - loss: 2.7253e-04 - mean_absolute_error: 0.0131 - val_loss: 1.4400e-04 - val_mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00013\n",
      "Epoch 76/350\n",
      "75/75 [==============================] - 17s 225ms/step - loss: 2.5005e-04 - mean_absolute_error: 0.0129 - val_loss: 1.4477e-04 - val_mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00013\n",
      "Epoch 77/350\n",
      "75/75 [==============================] - 17s 224ms/step - loss: 2.4968e-04 - mean_absolute_error: 0.0127 - val_loss: 1.4873e-04 - val_mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00013\n",
      "Epoch 78/350\n",
      "75/75 [==============================] - 17s 226ms/step - loss: 2.8274e-04 - mean_absolute_error: 0.0136 - val_loss: 1.2314e-04 - val_mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.00013 to 0.00012, saving model to results\\2021-06-21_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "Epoch 79/350\n",
      "75/75 [==============================] - 17s 225ms/step - loss: 2.2564e-04 - mean_absolute_error: 0.0124 - val_loss: 1.3175e-04 - val_mean_absolute_error: 0.0094\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00012\n",
      "Epoch 80/350\n",
      "75/75 [==============================] - 17s 227ms/step - loss: 2.5461e-04 - mean_absolute_error: 0.0129 - val_loss: 1.6453e-04 - val_mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00012\n",
      "Epoch 81/350\n",
      "75/75 [==============================] - 17s 223ms/step - loss: 2.5428e-04 - mean_absolute_error: 0.0128 - val_loss: 1.7429e-04 - val_mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00012\n",
      "Epoch 82/350\n",
      "75/75 [==============================] - 17s 224ms/step - loss: 2.6647e-04 - mean_absolute_error: 0.0130 - val_loss: 1.3994e-04 - val_mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00012\n",
      "Epoch 83/350\n",
      "75/75 [==============================] - 17s 224ms/step - loss: 2.4506e-04 - mean_absolute_error: 0.0128 - val_loss: 1.6183e-04 - val_mean_absolute_error: 0.0103\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00012\n",
      "Epoch 84/350\n",
      "75/75 [==============================] - 17s 225ms/step - loss: 2.5706e-04 - mean_absolute_error: 0.0130 - val_loss: 1.3349e-04 - val_mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00012\n",
      "Epoch 85/350\n",
      "75/75 [==============================] - 17s 223ms/step - loss: 2.4084e-04 - mean_absolute_error: 0.0126 - val_loss: 1.7988e-04 - val_mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00012\n",
      "Epoch 86/350\n",
      "75/75 [==============================] - 17s 225ms/step - loss: 2.4382e-04 - mean_absolute_error: 0.0127 - val_loss: 1.7463e-04 - val_mean_absolute_error: 0.0104\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00012\n",
      "Epoch 87/350\n",
      "75/75 [==============================] - 17s 225ms/step - loss: 2.4927e-04 - mean_absolute_error: 0.0127 - val_loss: 1.2569e-04 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00012\n",
      "Epoch 88/350\n",
      "75/75 [==============================] - 17s 227ms/step - loss: 2.5552e-04 - mean_absolute_error: 0.0129 - val_loss: 1.4695e-04 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00012\n",
      "Epoch 89/350\n",
      "75/75 [==============================] - 17s 224ms/step - loss: 2.4292e-04 - mean_absolute_error: 0.0128 - val_loss: 1.5770e-04 - val_mean_absolute_error: 0.0096\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00012\n",
      "Epoch 90/350\n",
      "75/75 [==============================] - 17s 223ms/step - loss: 2.8251e-04 - mean_absolute_error: 0.0135 - val_loss: 2.4554e-04 - val_mean_absolute_error: 0.0113\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00012\n",
      "Epoch 91/350\n",
      "75/75 [==============================] - 17s 225ms/step - loss: 2.5144e-04 - mean_absolute_error: 0.0129 - val_loss: 1.2418e-04 - val_mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00012\n",
      "Epoch 92/350\n",
      "75/75 [==============================] - 17s 230ms/step - loss: 2.4140e-04 - mean_absolute_error: 0.0127 - val_loss: 1.5464e-04 - val_mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00012\n",
      "Epoch 93/350\n",
      "75/75 [==============================] - 17s 225ms/step - loss: 2.4867e-04 - mean_absolute_error: 0.0128 - val_loss: 1.2664e-04 - val_mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00012\n",
      "Epoch 94/350\n",
      "75/75 [==============================] - 17s 227ms/step - loss: 2.5492e-04 - mean_absolute_error: 0.0129 - val_loss: 1.1887e-04 - val_mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.00012 to 0.00012, saving model to results\\2021-06-21_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "Epoch 95/350\n",
      "75/75 [==============================] - 17s 226ms/step - loss: 2.4320e-04 - mean_absolute_error: 0.0126 - val_loss: 1.2730e-04 - val_mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00012\n",
      "Epoch 96/350\n",
      "75/75 [==============================] - 17s 227ms/step - loss: 2.1851e-04 - mean_absolute_error: 0.0121 - val_loss: 1.2549e-04 - val_mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00012\n",
      "Epoch 97/350\n",
      "75/75 [==============================] - 18s 236ms/step - loss: 2.7251e-04 - mean_absolute_error: 0.0133 - val_loss: 1.2038e-04 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00012\n",
      "Epoch 98/350\n",
      "75/75 [==============================] - 17s 224ms/step - loss: 2.4006e-04 - mean_absolute_error: 0.0129 - val_loss: 1.3083e-04 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00012\n",
      "Epoch 99/350\n",
      "75/75 [==============================] - 18s 239ms/step - loss: 2.3064e-04 - mean_absolute_error: 0.0126 - val_loss: 1.5237e-04 - val_mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00012\n",
      "Epoch 100/350\n",
      "75/75 [==============================] - 19s 252ms/step - loss: 2.5744e-04 - mean_absolute_error: 0.0131 - val_loss: 1.4306e-04 - val_mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00012\n",
      "Epoch 101/350\n",
      "75/75 [==============================] - 17s 232ms/step - loss: 2.7511e-04 - mean_absolute_error: 0.0134 - val_loss: 2.2036e-04 - val_mean_absolute_error: 0.0109\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.00012\n",
      "Epoch 102/350\n",
      "75/75 [==============================] - 18s 240ms/step - loss: 2.5069e-04 - mean_absolute_error: 0.0131 - val_loss: 1.2680e-04 - val_mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.00012\n",
      "Epoch 103/350\n",
      "75/75 [==============================] - 18s 235ms/step - loss: 2.3633e-04 - mean_absolute_error: 0.0124 - val_loss: 1.2031e-04 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.00012\n",
      "Epoch 104/350\n",
      "75/75 [==============================] - 18s 234ms/step - loss: 2.3636e-04 - mean_absolute_error: 0.0123 - val_loss: 2.1188e-04 - val_mean_absolute_error: 0.0104\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.00012\n",
      "Epoch 105/350\n",
      "75/75 [==============================] - 17s 229ms/step - loss: 2.5773e-04 - mean_absolute_error: 0.0128 - val_loss: 1.2620e-04 - val_mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.00012\n",
      "Epoch 106/350\n",
      "75/75 [==============================] - 18s 236ms/step - loss: 2.4340e-04 - mean_absolute_error: 0.0126 - val_loss: 1.4752e-04 - val_mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.00012\n",
      "Epoch 107/350\n",
      "75/75 [==============================] - 17s 233ms/step - loss: 2.7211e-04 - mean_absolute_error: 0.0132 - val_loss: 1.2807e-04 - val_mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.00012\n",
      "Epoch 108/350\n",
      "75/75 [==============================] - 17s 228ms/step - loss: 2.2933e-04 - mean_absolute_error: 0.0124 - val_loss: 1.2067e-04 - val_mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.00012\n",
      "Epoch 109/350\n",
      "75/75 [==============================] - 17s 230ms/step - loss: 2.2985e-04 - mean_absolute_error: 0.0125 - val_loss: 1.0580e-04 - val_mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.00012 to 0.00011, saving model to results\\2021-06-21_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "Epoch 110/350\n",
      "75/75 [==============================] - 17s 233ms/step - loss: 2.5629e-04 - mean_absolute_error: 0.0131 - val_loss: 1.3815e-04 - val_mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.00011\n",
      "Epoch 111/350\n",
      "75/75 [==============================] - 18s 238ms/step - loss: 2.4155e-04 - mean_absolute_error: 0.0128 - val_loss: 1.3799e-04 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.00011\n",
      "Epoch 112/350\n",
      "75/75 [==============================] - 17s 231ms/step - loss: 2.4867e-04 - mean_absolute_error: 0.0128 - val_loss: 1.1275e-04 - val_mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.00011\n",
      "Epoch 113/350\n",
      "75/75 [==============================] - 17s 224ms/step - loss: 2.3923e-04 - mean_absolute_error: 0.0127 - val_loss: 1.3284e-04 - val_mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.00011\n",
      "Epoch 114/350\n",
      "75/75 [==============================] - 17s 225ms/step - loss: 2.4766e-04 - mean_absolute_error: 0.0127 - val_loss: 1.5129e-04 - val_mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.00011\n",
      "Epoch 115/350\n",
      "75/75 [==============================] - 17s 228ms/step - loss: 2.3292e-04 - mean_absolute_error: 0.0124 - val_loss: 1.1467e-04 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.00011\n",
      "Epoch 116/350\n",
      "75/75 [==============================] - 17s 228ms/step - loss: 2.2200e-04 - mean_absolute_error: 0.0120 - val_loss: 1.3242e-04 - val_mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.00011\n",
      "Epoch 117/350\n",
      "75/75 [==============================] - 17s 231ms/step - loss: 2.3411e-04 - mean_absolute_error: 0.0122 - val_loss: 1.1645e-04 - val_mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.00011\n",
      "Epoch 118/350\n",
      "75/75 [==============================] - 18s 241ms/step - loss: 2.3571e-04 - mean_absolute_error: 0.0125 - val_loss: 1.6566e-04 - val_mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.00011\n",
      "Epoch 119/350\n",
      "75/75 [==============================] - 17s 226ms/step - loss: 2.4928e-04 - mean_absolute_error: 0.0126 - val_loss: 1.1333e-04 - val_mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.00011\n",
      "Epoch 120/350\n",
      "75/75 [==============================] - 17s 231ms/step - loss: 2.2684e-04 - mean_absolute_error: 0.0123 - val_loss: 1.1455e-04 - val_mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.00011\n",
      "Epoch 121/350\n",
      "75/75 [==============================] - 18s 242ms/step - loss: 2.3830e-04 - mean_absolute_error: 0.0127 - val_loss: 1.5604e-04 - val_mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.00011\n",
      "Epoch 122/350\n",
      "75/75 [==============================] - 18s 237ms/step - loss: 2.3736e-04 - mean_absolute_error: 0.0127 - val_loss: 1.2207e-04 - val_mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.00011\n",
      "Epoch 123/350\n",
      "75/75 [==============================] - 18s 241ms/step - loss: 2.2194e-04 - mean_absolute_error: 0.0120 - val_loss: 1.1941e-04 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.00011\n",
      "Epoch 124/350\n",
      "75/75 [==============================] - 18s 234ms/step - loss: 2.2362e-04 - mean_absolute_error: 0.0124 - val_loss: 1.4540e-04 - val_mean_absolute_error: 0.0099\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.00011\n",
      "Epoch 125/350\n",
      "75/75 [==============================] - 18s 237ms/step - loss: 2.3590e-04 - mean_absolute_error: 0.0123 - val_loss: 1.2449e-04 - val_mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.00011\n",
      "Epoch 126/350\n",
      "75/75 [==============================] - 18s 239ms/step - loss: 2.2941e-04 - mean_absolute_error: 0.0123 - val_loss: 1.1950e-04 - val_mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.00011\n",
      "Epoch 127/350\n",
      "75/75 [==============================] - 17s 224ms/step - loss: 2.2170e-04 - mean_absolute_error: 0.0123 - val_loss: 1.3403e-04 - val_mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.00011\n",
      "Epoch 128/350\n",
      "75/75 [==============================] - 17s 223ms/step - loss: 2.3639e-04 - mean_absolute_error: 0.0125 - val_loss: 1.2020e-04 - val_mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.00011\n",
      "Epoch 129/350\n",
      "75/75 [==============================] - 17s 224ms/step - loss: 2.3591e-04 - mean_absolute_error: 0.0127 - val_loss: 1.1266e-04 - val_mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.00011\n",
      "Epoch 130/350\n",
      "75/75 [==============================] - 17s 225ms/step - loss: 2.5329e-04 - mean_absolute_error: 0.0128 - val_loss: 1.2427e-04 - val_mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.00011\n",
      "Epoch 131/350\n",
      "75/75 [==============================] - 17s 226ms/step - loss: 2.5612e-04 - mean_absolute_error: 0.0128 - val_loss: 1.6861e-04 - val_mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.00011\n",
      "Epoch 132/350\n",
      "75/75 [==============================] - 17s 226ms/step - loss: 2.2568e-04 - mean_absolute_error: 0.0122 - val_loss: 1.1706e-04 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.00011\n",
      "Epoch 133/350\n",
      "75/75 [==============================] - 17s 226ms/step - loss: 2.3027e-04 - mean_absolute_error: 0.0125 - val_loss: 1.1612e-04 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.00011\n",
      "Epoch 134/350\n",
      "75/75 [==============================] - 17s 228ms/step - loss: 2.4224e-04 - mean_absolute_error: 0.0125 - val_loss: 1.4521e-04 - val_mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.00011\n",
      "Epoch 135/350\n",
      "75/75 [==============================] - 17s 225ms/step - loss: 2.1257e-04 - mean_absolute_error: 0.0120 - val_loss: 1.1896e-04 - val_mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.00011\n",
      "Epoch 136/350\n",
      "75/75 [==============================] - 17s 226ms/step - loss: 2.1341e-04 - mean_absolute_error: 0.0120 - val_loss: 1.7727e-04 - val_mean_absolute_error: 0.0098\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.00011\n",
      "Epoch 137/350\n",
      "75/75 [==============================] - 17s 226ms/step - loss: 2.2148e-04 - mean_absolute_error: 0.0121 - val_loss: 1.1543e-04 - val_mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.00011\n",
      "Epoch 138/350\n",
      "75/75 [==============================] - 17s 226ms/step - loss: 2.3754e-04 - mean_absolute_error: 0.0123 - val_loss: 1.3756e-04 - val_mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.00011\n",
      "Epoch 139/350\n",
      "75/75 [==============================] - 17s 225ms/step - loss: 2.5796e-04 - mean_absolute_error: 0.0131 - val_loss: 1.2405e-04 - val_mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.00011\n",
      "Epoch 140/350\n",
      "75/75 [==============================] - 18s 238ms/step - loss: 2.0252e-04 - mean_absolute_error: 0.0118 - val_loss: 1.0353e-04 - val_mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.00011 to 0.00010, saving model to results\\2021-06-21_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "Epoch 141/350\n",
      "75/75 [==============================] - 18s 235ms/step - loss: 2.0868e-04 - mean_absolute_error: 0.0119 - val_loss: 1.4603e-04 - val_mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.00010\n",
      "Epoch 142/350\n",
      "75/75 [==============================] - 17s 230ms/step - loss: 2.3407e-04 - mean_absolute_error: 0.0123 - val_loss: 1.0400e-04 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.00010\n",
      "Epoch 143/350\n",
      "75/75 [==============================] - 17s 224ms/step - loss: 2.0028e-04 - mean_absolute_error: 0.0117 - val_loss: 1.1866e-04 - val_mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.00010\n",
      "Epoch 144/350\n",
      "75/75 [==============================] - 17s 224ms/step - loss: 2.4379e-04 - mean_absolute_error: 0.0127 - val_loss: 1.0900e-04 - val_mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.00010\n",
      "Epoch 145/350\n",
      "75/75 [==============================] - 17s 227ms/step - loss: 2.2796e-04 - mean_absolute_error: 0.0124 - val_loss: 1.0542e-04 - val_mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.00010\n",
      "Epoch 146/350\n",
      "75/75 [==============================] - 17s 226ms/step - loss: 2.3876e-04 - mean_absolute_error: 0.0126 - val_loss: 1.1461e-04 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.00010\n",
      "Epoch 147/350\n",
      "75/75 [==============================] - 17s 232ms/step - loss: 2.2242e-04 - mean_absolute_error: 0.0122 - val_loss: 2.1461e-04 - val_mean_absolute_error: 0.0108\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.00010\n",
      "Epoch 148/350\n",
      "75/75 [==============================] - 17s 229ms/step - loss: 2.1565e-04 - mean_absolute_error: 0.0121 - val_loss: 1.1386e-04 - val_mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.00010\n",
      "Epoch 149/350\n",
      "75/75 [==============================] - 17s 229ms/step - loss: 2.2067e-04 - mean_absolute_error: 0.0120 - val_loss: 1.4611e-04 - val_mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.00010\n",
      "Epoch 150/350\n",
      "75/75 [==============================] - 17s 224ms/step - loss: 2.2600e-04 - mean_absolute_error: 0.0122 - val_loss: 1.0924e-04 - val_mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.00010\n",
      "Epoch 151/350\n",
      "75/75 [==============================] - 17s 224ms/step - loss: 2.5759e-04 - mean_absolute_error: 0.0129 - val_loss: 1.7775e-04 - val_mean_absolute_error: 0.0096\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.00010\n",
      "Epoch 152/350\n",
      "75/75 [==============================] - 17s 225ms/step - loss: 2.3908e-04 - mean_absolute_error: 0.0125 - val_loss: 1.6960e-04 - val_mean_absolute_error: 0.0098\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.00010\n",
      "Epoch 153/350\n",
      "75/75 [==============================] - 17s 226ms/step - loss: 2.0989e-04 - mean_absolute_error: 0.0122 - val_loss: 1.0441e-04 - val_mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.00010\n",
      "Epoch 154/350\n",
      "75/75 [==============================] - 17s 224ms/step - loss: 2.0656e-04 - mean_absolute_error: 0.0119 - val_loss: 1.0925e-04 - val_mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.00010\n",
      "Epoch 155/350\n",
      "75/75 [==============================] - 17s 225ms/step - loss: 2.5518e-04 - mean_absolute_error: 0.0126 - val_loss: 1.0993e-04 - val_mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.00010\n",
      "Epoch 156/350\n",
      "75/75 [==============================] - 17s 226ms/step - loss: 2.1443e-04 - mean_absolute_error: 0.0120 - val_loss: 1.0549e-04 - val_mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.00010\n",
      "Epoch 157/350\n",
      "75/75 [==============================] - 17s 227ms/step - loss: 2.2262e-04 - mean_absolute_error: 0.0120 - val_loss: 1.5937e-04 - val_mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.00010\n",
      "Epoch 158/350\n",
      "75/75 [==============================] - 17s 226ms/step - loss: 2.3977e-04 - mean_absolute_error: 0.0124 - val_loss: 1.3825e-04 - val_mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.00010\n",
      "Epoch 159/350\n",
      "75/75 [==============================] - 17s 227ms/step - loss: 2.1983e-04 - mean_absolute_error: 0.0122 - val_loss: 1.1587e-04 - val_mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.00010\n",
      "Epoch 160/350\n",
      "75/75 [==============================] - 18s 235ms/step - loss: 2.2633e-04 - mean_absolute_error: 0.0122 - val_loss: 1.1975e-04 - val_mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.00010\n",
      "Epoch 161/350\n",
      "75/75 [==============================] - 17s 234ms/step - loss: 2.2441e-04 - mean_absolute_error: 0.0121 - val_loss: 1.0287e-04 - val_mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.00010 to 0.00010, saving model to results\\2021-06-21_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "Epoch 162/350\n",
      "75/75 [==============================] - 17s 225ms/step - loss: 2.1222e-04 - mean_absolute_error: 0.0121 - val_loss: 1.0660e-04 - val_mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.00010\n",
      "Epoch 163/350\n",
      "75/75 [==============================] - 17s 225ms/step - loss: 2.0934e-04 - mean_absolute_error: 0.0116 - val_loss: 1.2283e-04 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.00010\n",
      "Epoch 164/350\n",
      "75/75 [==============================] - 17s 229ms/step - loss: 2.1715e-04 - mean_absolute_error: 0.0119 - val_loss: 2.0400e-04 - val_mean_absolute_error: 0.0103\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.00010\n",
      "Epoch 165/350\n",
      "75/75 [==============================] - 17s 225ms/step - loss: 2.4046e-04 - mean_absolute_error: 0.0125 - val_loss: 1.6040e-04 - val_mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.00010\n",
      "Epoch 166/350\n",
      "75/75 [==============================] - 17s 228ms/step - loss: 2.2766e-04 - mean_absolute_error: 0.0122 - val_loss: 1.1045e-04 - val_mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.00010\n",
      "Epoch 167/350\n",
      "75/75 [==============================] - 17s 229ms/step - loss: 2.3269e-04 - mean_absolute_error: 0.0123 - val_loss: 1.5351e-04 - val_mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.00010\n",
      "Epoch 168/350\n",
      "75/75 [==============================] - 17s 227ms/step - loss: 2.1857e-04 - mean_absolute_error: 0.0119 - val_loss: 1.0166e-04 - val_mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.00010 to 0.00010, saving model to results\\2021-06-21_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "Epoch 169/350\n",
      "75/75 [==============================] - 17s 226ms/step - loss: 2.1734e-04 - mean_absolute_error: 0.0122 - val_loss: 1.3455e-04 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.00010\n",
      "Epoch 170/350\n",
      "75/75 [==============================] - 17s 226ms/step - loss: 2.0730e-04 - mean_absolute_error: 0.0118 - val_loss: 1.4673e-04 - val_mean_absolute_error: 0.0098\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.00010\n",
      "Epoch 171/350\n",
      "75/75 [==============================] - 17s 227ms/step - loss: 2.3626e-04 - mean_absolute_error: 0.0123 - val_loss: 1.0299e-04 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.00010\n",
      "Epoch 172/350\n",
      "75/75 [==============================] - 17s 228ms/step - loss: 2.1940e-04 - mean_absolute_error: 0.0121 - val_loss: 1.2700e-04 - val_mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.00010\n",
      "Epoch 173/350\n",
      "75/75 [==============================] - 17s 227ms/step - loss: 2.5069e-04 - mean_absolute_error: 0.0125 - val_loss: 2.1252e-04 - val_mean_absolute_error: 0.0111\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.00010\n",
      "Epoch 174/350\n",
      "75/75 [==============================] - 17s 230ms/step - loss: 2.2604e-04 - mean_absolute_error: 0.0122 - val_loss: 1.3122e-04 - val_mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.00010\n",
      "Epoch 175/350\n",
      "75/75 [==============================] - 17s 229ms/step - loss: 2.0287e-04 - mean_absolute_error: 0.0117 - val_loss: 1.0277e-04 - val_mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.00010\n",
      "Epoch 176/350\n",
      "75/75 [==============================] - 17s 231ms/step - loss: 2.2407e-04 - mean_absolute_error: 0.0122 - val_loss: 1.7301e-04 - val_mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.00010\n",
      "Epoch 177/350\n",
      "75/75 [==============================] - 17s 227ms/step - loss: 2.4294e-04 - mean_absolute_error: 0.0126 - val_loss: 1.1832e-04 - val_mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.00010\n",
      "Epoch 178/350\n",
      "75/75 [==============================] - 17s 226ms/step - loss: 2.1261e-04 - mean_absolute_error: 0.0120 - val_loss: 1.2054e-04 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.00010\n",
      "Epoch 179/350\n",
      "75/75 [==============================] - 17s 226ms/step - loss: 2.2637e-04 - mean_absolute_error: 0.0121 - val_loss: 9.9026e-05 - val_mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 00179: val_loss improved from 0.00010 to 0.00010, saving model to results\\2021-06-21_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "Epoch 180/350\n",
      "75/75 [==============================] - 17s 227ms/step - loss: 2.0135e-04 - mean_absolute_error: 0.0116 - val_loss: 1.0828e-04 - val_mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.00010\n",
      "Epoch 181/350\n",
      "75/75 [==============================] - 17s 227ms/step - loss: 2.1574e-04 - mean_absolute_error: 0.0120 - val_loss: 1.3032e-04 - val_mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.00010\n",
      "Epoch 182/350\n",
      "75/75 [==============================] - 17s 227ms/step - loss: 2.2473e-04 - mean_absolute_error: 0.0121 - val_loss: 1.0900e-04 - val_mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.00010\n",
      "Epoch 183/350\n",
      "75/75 [==============================] - 17s 226ms/step - loss: 2.2852e-04 - mean_absolute_error: 0.0122 - val_loss: 1.0833e-04 - val_mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.00010\n",
      "Epoch 184/350\n",
      "75/75 [==============================] - 17s 229ms/step - loss: 2.1585e-04 - mean_absolute_error: 0.0119 - val_loss: 1.8753e-04 - val_mean_absolute_error: 0.0104\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.00010\n",
      "Epoch 185/350\n",
      "75/75 [==============================] - 17s 232ms/step - loss: 2.2121e-04 - mean_absolute_error: 0.0121 - val_loss: 1.2696e-04 - val_mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.00010\n",
      "Epoch 186/350\n",
      "75/75 [==============================] - 17s 225ms/step - loss: 2.0206e-04 - mean_absolute_error: 0.0117 - val_loss: 1.1402e-04 - val_mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.00010\n",
      "Epoch 187/350\n",
      "75/75 [==============================] - 17s 226ms/step - loss: 2.1527e-04 - mean_absolute_error: 0.0120 - val_loss: 1.2291e-04 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.00010\n",
      "Epoch 188/350\n",
      "75/75 [==============================] - 18s 247ms/step - loss: 2.2802e-04 - mean_absolute_error: 0.0119 - val_loss: 1.2713e-04 - val_mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.00010\n",
      "Epoch 189/350\n",
      "75/75 [==============================] - 18s 238ms/step - loss: 2.1861e-04 - mean_absolute_error: 0.0117 - val_loss: 1.3697e-04 - val_mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.00010\n",
      "Epoch 190/350\n",
      "75/75 [==============================] - 17s 231ms/step - loss: 2.3813e-04 - mean_absolute_error: 0.0122 - val_loss: 1.6522e-04 - val_mean_absolute_error: 0.0099\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.00010\n",
      "Epoch 191/350\n",
      "75/75 [==============================] - 17s 231ms/step - loss: 2.2247e-04 - mean_absolute_error: 0.0121 - val_loss: 1.1872e-04 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.00010\n",
      "Epoch 192/350\n",
      "75/75 [==============================] - 17s 228ms/step - loss: 2.0780e-04 - mean_absolute_error: 0.0116 - val_loss: 1.1404e-04 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.00010\n",
      "Epoch 193/350\n",
      "75/75 [==============================] - 17s 232ms/step - loss: 2.2556e-04 - mean_absolute_error: 0.0119 - val_loss: 1.1390e-04 - val_mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.00010\n",
      "Epoch 194/350\n",
      "75/75 [==============================] - 18s 235ms/step - loss: 2.1305e-04 - mean_absolute_error: 0.0117 - val_loss: 1.8155e-04 - val_mean_absolute_error: 0.0098\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.00010\n",
      "Epoch 195/350\n",
      "75/75 [==============================] - 17s 233ms/step - loss: 1.9936e-04 - mean_absolute_error: 0.0115 - val_loss: 1.1468e-04 - val_mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.00010\n",
      "Epoch 196/350\n",
      "75/75 [==============================] - 18s 237ms/step - loss: 2.0101e-04 - mean_absolute_error: 0.0116 - val_loss: 1.1357e-04 - val_mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.00010\n",
      "Epoch 197/350\n",
      "75/75 [==============================] - 17s 228ms/step - loss: 2.3327e-04 - mean_absolute_error: 0.0122 - val_loss: 1.3226e-04 - val_mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.00010\n",
      "Epoch 198/350\n",
      "75/75 [==============================] - 17s 228ms/step - loss: 2.4293e-04 - mean_absolute_error: 0.0124 - val_loss: 1.4925e-04 - val_mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.00010\n",
      "Epoch 199/350\n",
      "75/75 [==============================] - 17s 230ms/step - loss: 2.1727e-04 - mean_absolute_error: 0.0117 - val_loss: 1.2970e-04 - val_mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.00010\n",
      "Epoch 200/350\n",
      "75/75 [==============================] - 17s 228ms/step - loss: 2.1688e-04 - mean_absolute_error: 0.0118 - val_loss: 1.3937e-04 - val_mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.00010\n",
      "Epoch 201/350\n",
      "75/75 [==============================] - 18s 246ms/step - loss: 2.1364e-04 - mean_absolute_error: 0.0117 - val_loss: 1.2046e-04 - val_mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.00010\n",
      "Epoch 202/350\n",
      "75/75 [==============================] - 18s 235ms/step - loss: 2.1104e-04 - mean_absolute_error: 0.0118 - val_loss: 1.0794e-04 - val_mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.00010\n",
      "Epoch 203/350\n",
      "75/75 [==============================] - 17s 230ms/step - loss: 2.2050e-04 - mean_absolute_error: 0.0117 - val_loss: 1.4349e-04 - val_mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.00010\n",
      "Epoch 204/350\n",
      "75/75 [==============================] - 17s 232ms/step - loss: 2.0240e-04 - mean_absolute_error: 0.0117 - val_loss: 1.0459e-04 - val_mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.00010\n",
      "Epoch 205/350\n",
      "75/75 [==============================] - 17s 231ms/step - loss: 1.9436e-04 - mean_absolute_error: 0.0113 - val_loss: 1.1632e-04 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.00010\n",
      "Epoch 206/350\n",
      "75/75 [==============================] - 17s 231ms/step - loss: 2.1520e-04 - mean_absolute_error: 0.0116 - val_loss: 1.6138e-04 - val_mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.00010\n",
      "Epoch 207/350\n",
      "75/75 [==============================] - 17s 231ms/step - loss: 2.0593e-04 - mean_absolute_error: 0.0118 - val_loss: 1.0437e-04 - val_mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.00010\n",
      "Epoch 208/350\n",
      "75/75 [==============================] - 18s 239ms/step - loss: 2.0311e-04 - mean_absolute_error: 0.0118 - val_loss: 9.9280e-05 - val_mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.00010\n",
      "Epoch 209/350\n",
      "75/75 [==============================] - 17s 233ms/step - loss: 2.0628e-04 - mean_absolute_error: 0.0116 - val_loss: 1.4127e-04 - val_mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.00010\n",
      "Epoch 210/350\n",
      "75/75 [==============================] - 17s 233ms/step - loss: 2.1932e-04 - mean_absolute_error: 0.0119 - val_loss: 1.6382e-04 - val_mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.00010\n",
      "Epoch 211/350\n",
      "75/75 [==============================] - 17s 230ms/step - loss: 2.1889e-04 - mean_absolute_error: 0.0118 - val_loss: 1.3550e-04 - val_mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.00010\n",
      "Epoch 212/350\n",
      "75/75 [==============================] - 17s 229ms/step - loss: 2.2034e-04 - mean_absolute_error: 0.0119 - val_loss: 1.1337e-04 - val_mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.00010\n",
      "Epoch 213/350\n",
      "75/75 [==============================] - 17s 229ms/step - loss: 2.1822e-04 - mean_absolute_error: 0.0117 - val_loss: 1.0185e-04 - val_mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.00010\n",
      "Epoch 214/350\n",
      "75/75 [==============================] - 17s 232ms/step - loss: 2.1310e-04 - mean_absolute_error: 0.0117 - val_loss: 1.0586e-04 - val_mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.00010\n",
      "Epoch 215/350\n",
      "75/75 [==============================] - 18s 234ms/step - loss: 2.0815e-04 - mean_absolute_error: 0.0117 - val_loss: 9.9280e-05 - val_mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.00010\n",
      "Epoch 216/350\n",
      "75/75 [==============================] - 17s 230ms/step - loss: 2.0409e-04 - mean_absolute_error: 0.0114 - val_loss: 1.0146e-04 - val_mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.00010\n",
      "Epoch 217/350\n",
      "75/75 [==============================] - 17s 231ms/step - loss: 2.0213e-04 - mean_absolute_error: 0.0114 - val_loss: 1.1661e-04 - val_mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.00010\n",
      "Epoch 218/350\n",
      "75/75 [==============================] - 17s 232ms/step - loss: 2.1084e-04 - mean_absolute_error: 0.0116 - val_loss: 1.1000e-04 - val_mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.00010\n",
      "Epoch 219/350\n",
      "75/75 [==============================] - 17s 233ms/step - loss: 2.2338e-04 - mean_absolute_error: 0.0119 - val_loss: 1.1040e-04 - val_mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.00010\n",
      "Epoch 220/350\n",
      "75/75 [==============================] - 17s 232ms/step - loss: 2.2216e-04 - mean_absolute_error: 0.0119 - val_loss: 1.2116e-04 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.00010\n",
      "Epoch 221/350\n",
      "75/75 [==============================] - 17s 229ms/step - loss: 2.2852e-04 - mean_absolute_error: 0.0121 - val_loss: 1.0405e-04 - val_mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.00010\n",
      "Epoch 222/350\n",
      "75/75 [==============================] - 17s 229ms/step - loss: 2.2279e-04 - mean_absolute_error: 0.0119 - val_loss: 1.2960e-04 - val_mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.00010\n",
      "Epoch 223/350\n",
      "75/75 [==============================] - 17s 228ms/step - loss: 2.1967e-04 - mean_absolute_error: 0.0118 - val_loss: 1.1860e-04 - val_mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.00010\n",
      "Epoch 224/350\n",
      "75/75 [==============================] - 17s 229ms/step - loss: 1.9575e-04 - mean_absolute_error: 0.0114 - val_loss: 1.0470e-04 - val_mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.00010\n",
      "Epoch 225/350\n",
      "75/75 [==============================] - 17s 230ms/step - loss: 2.1564e-04 - mean_absolute_error: 0.0117 - val_loss: 1.0581e-04 - val_mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.00010\n",
      "Epoch 226/350\n",
      "75/75 [==============================] - 18s 240ms/step - loss: 2.1849e-04 - mean_absolute_error: 0.0118 - val_loss: 1.3922e-04 - val_mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.00010\n",
      "Epoch 227/350\n",
      "75/75 [==============================] - 17s 228ms/step - loss: 2.2588e-04 - mean_absolute_error: 0.0120 - val_loss: 1.3305e-04 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.00010\n",
      "Epoch 228/350\n",
      "75/75 [==============================] - 17s 228ms/step - loss: 2.1819e-04 - mean_absolute_error: 0.0118 - val_loss: 1.1290e-04 - val_mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.00010\n",
      "Epoch 229/350\n",
      "75/75 [==============================] - 17s 230ms/step - loss: 2.1911e-04 - mean_absolute_error: 0.0118 - val_loss: 1.0059e-04 - val_mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.00010\n",
      "Epoch 230/350\n",
      "75/75 [==============================] - 17s 228ms/step - loss: 2.0232e-04 - mean_absolute_error: 0.0114 - val_loss: 1.0962e-04 - val_mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.00010\n",
      "Epoch 231/350\n",
      "75/75 [==============================] - 18s 242ms/step - loss: 2.2057e-04 - mean_absolute_error: 0.0118 - val_loss: 1.0719e-04 - val_mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.00010\n",
      "Epoch 232/350\n",
      "75/75 [==============================] - 17s 230ms/step - loss: 2.1243e-04 - mean_absolute_error: 0.0117 - val_loss: 1.2210e-04 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.00010\n",
      "Epoch 233/350\n",
      "75/75 [==============================] - 19s 250ms/step - loss: 2.1178e-04 - mean_absolute_error: 0.0117 - val_loss: 1.0780e-04 - val_mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.00010\n",
      "Epoch 234/350\n",
      "75/75 [==============================] - 19s 255ms/step - loss: 2.1271e-04 - mean_absolute_error: 0.0115 - val_loss: 1.1115e-04 - val_mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.00010\n",
      "Epoch 235/350\n",
      "75/75 [==============================] - 17s 230ms/step - loss: 2.1488e-04 - mean_absolute_error: 0.0117 - val_loss: 1.1041e-04 - val_mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.00010\n",
      "Epoch 236/350\n",
      "75/75 [==============================] - 18s 234ms/step - loss: 1.9627e-04 - mean_absolute_error: 0.0113 - val_loss: 1.0811e-04 - val_mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.00010\n",
      "Epoch 237/350\n",
      "75/75 [==============================] - 17s 232ms/step - loss: 2.3157e-04 - mean_absolute_error: 0.0120 - val_loss: 1.0278e-04 - val_mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.00010\n",
      "Epoch 238/350\n",
      "75/75 [==============================] - 17s 223ms/step - loss: 2.0051e-04 - mean_absolute_error: 0.0114 - val_loss: 1.0146e-04 - val_mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.00010\n",
      "Epoch 239/350\n",
      "75/75 [==============================] - 17s 221ms/step - loss: 2.0906e-04 - mean_absolute_error: 0.0117 - val_loss: 9.8820e-05 - val_mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 00239: val_loss improved from 0.00010 to 0.00010, saving model to results\\2021-06-21_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "Epoch 240/350\n",
      "75/75 [==============================] - 17s 226ms/step - loss: 2.1915e-04 - mean_absolute_error: 0.0118 - val_loss: 1.0609e-04 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.00010\n",
      "Epoch 241/350\n",
      "75/75 [==============================] - 17s 226ms/step - loss: 1.9687e-04 - mean_absolute_error: 0.0113 - val_loss: 9.8937e-05 - val_mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.00010\n",
      "Epoch 242/350\n",
      "75/75 [==============================] - 17s 225ms/step - loss: 2.0380e-04 - mean_absolute_error: 0.0116 - val_loss: 1.2996e-04 - val_mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.00010\n",
      "Epoch 243/350\n",
      "75/75 [==============================] - 17s 225ms/step - loss: 2.2364e-04 - mean_absolute_error: 0.0119 - val_loss: 1.2331e-04 - val_mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.00010\n",
      "Epoch 244/350\n",
      "75/75 [==============================] - 17s 222ms/step - loss: 2.0263e-04 - mean_absolute_error: 0.0114 - val_loss: 1.0294e-04 - val_mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.00010\n",
      "Epoch 245/350\n",
      "75/75 [==============================] - 16s 218ms/step - loss: 2.0375e-04 - mean_absolute_error: 0.0116 - val_loss: 1.0668e-04 - val_mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.00010\n",
      "Epoch 246/350\n",
      "75/75 [==============================] - 17s 229ms/step - loss: 2.0691e-04 - mean_absolute_error: 0.0114 - val_loss: 1.4090e-04 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.00010\n",
      "Epoch 247/350\n",
      "75/75 [==============================] - 17s 223ms/step - loss: 2.1411e-04 - mean_absolute_error: 0.0117 - val_loss: 1.0177e-04 - val_mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.00010\n",
      "Epoch 248/350\n",
      "75/75 [==============================] - 17s 223ms/step - loss: 2.2291e-04 - mean_absolute_error: 0.0118 - val_loss: 1.1370e-04 - val_mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.00010\n",
      "Epoch 249/350\n",
      "75/75 [==============================] - 16s 219ms/step - loss: 2.2059e-04 - mean_absolute_error: 0.0121 - val_loss: 1.2217e-04 - val_mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.00010\n",
      "Epoch 250/350\n",
      "75/75 [==============================] - 17s 221ms/step - loss: 1.9794e-04 - mean_absolute_error: 0.0114 - val_loss: 1.0132e-04 - val_mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.00010\n",
      "Epoch 251/350\n",
      "75/75 [==============================] - 17s 221ms/step - loss: 2.2179e-04 - mean_absolute_error: 0.0119 - val_loss: 1.0579e-04 - val_mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.00010\n",
      "Epoch 252/350\n",
      "75/75 [==============================] - 17s 232ms/step - loss: 2.0358e-04 - mean_absolute_error: 0.0115 - val_loss: 1.1920e-04 - val_mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.00010\n",
      "Epoch 253/350\n",
      "75/75 [==============================] - 17s 228ms/step - loss: 1.9664e-04 - mean_absolute_error: 0.0114 - val_loss: 1.0412e-04 - val_mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.00010\n",
      "Epoch 254/350\n",
      "75/75 [==============================] - 17s 229ms/step - loss: 2.0989e-04 - mean_absolute_error: 0.0116 - val_loss: 1.1807e-04 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.00010\n",
      "Epoch 255/350\n",
      "75/75 [==============================] - 17s 226ms/step - loss: 2.0281e-04 - mean_absolute_error: 0.0115 - val_loss: 1.1220e-04 - val_mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.00010\n",
      "Epoch 256/350\n",
      "75/75 [==============================] - 17s 229ms/step - loss: 2.0229e-04 - mean_absolute_error: 0.0117 - val_loss: 1.0327e-04 - val_mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.00010\n",
      "Epoch 257/350\n",
      "75/75 [==============================] - 17s 225ms/step - loss: 2.1701e-04 - mean_absolute_error: 0.0115 - val_loss: 1.1002e-04 - val_mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.00010\n",
      "Epoch 258/350\n",
      "75/75 [==============================] - 18s 235ms/step - loss: 2.1529e-04 - mean_absolute_error: 0.0117 - val_loss: 1.1767e-04 - val_mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.00010\n",
      "Epoch 259/350\n",
      "75/75 [==============================] - 17s 232ms/step - loss: 1.8783e-04 - mean_absolute_error: 0.0111 - val_loss: 9.9197e-05 - val_mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.00010\n",
      "Epoch 260/350\n",
      "75/75 [==============================] - 17s 230ms/step - loss: 2.0161e-04 - mean_absolute_error: 0.0115 - val_loss: 1.0900e-04 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.00010\n",
      "Epoch 261/350\n",
      "75/75 [==============================] - 17s 224ms/step - loss: 2.0566e-04 - mean_absolute_error: 0.0114 - val_loss: 1.0502e-04 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.00010\n",
      "Epoch 262/350\n",
      "75/75 [==============================] - 17s 225ms/step - loss: 1.9924e-04 - mean_absolute_error: 0.0115 - val_loss: 1.0661e-04 - val_mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.00010\n",
      "Epoch 263/350\n",
      "75/75 [==============================] - 17s 226ms/step - loss: 2.0270e-04 - mean_absolute_error: 0.0115 - val_loss: 1.3703e-04 - val_mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.00010\n",
      "Epoch 264/350\n",
      "75/75 [==============================] - 17s 230ms/step - loss: 1.9425e-04 - mean_absolute_error: 0.0113 - val_loss: 1.2985e-04 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.00010\n",
      "Epoch 265/350\n",
      "75/75 [==============================] - 17s 223ms/step - loss: 1.9393e-04 - mean_absolute_error: 0.0113 - val_loss: 1.0139e-04 - val_mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.00010\n",
      "Epoch 266/350\n",
      "75/75 [==============================] - 17s 227ms/step - loss: 1.9318e-04 - mean_absolute_error: 0.0113 - val_loss: 1.0489e-04 - val_mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.00010\n",
      "Epoch 267/350\n",
      "75/75 [==============================] - 18s 234ms/step - loss: 2.0806e-04 - mean_absolute_error: 0.0114 - val_loss: 1.0450e-04 - val_mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.00010\n",
      "Epoch 268/350\n",
      "75/75 [==============================] - 17s 231ms/step - loss: 2.0179e-04 - mean_absolute_error: 0.0113 - val_loss: 1.0913e-04 - val_mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.00010\n",
      "Epoch 269/350\n",
      "75/75 [==============================] - 17s 230ms/step - loss: 2.1024e-04 - mean_absolute_error: 0.0116 - val_loss: 1.0033e-04 - val_mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.00010\n",
      "Epoch 270/350\n",
      "75/75 [==============================] - 19s 249ms/step - loss: 2.1075e-04 - mean_absolute_error: 0.0115 - val_loss: 1.0048e-04 - val_mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.00010\n",
      "Epoch 271/350\n",
      "75/75 [==============================] - 19s 254ms/step - loss: 1.8264e-04 - mean_absolute_error: 0.0109 - val_loss: 1.1192e-04 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.00010\n",
      "Epoch 272/350\n",
      "75/75 [==============================] - 18s 238ms/step - loss: 2.1913e-04 - mean_absolute_error: 0.0118 - val_loss: 1.2287e-04 - val_mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.00010\n",
      "Epoch 273/350\n",
      "75/75 [==============================] - 18s 239ms/step - loss: 2.0436e-04 - mean_absolute_error: 0.0114 - val_loss: 1.0328e-04 - val_mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.00010\n",
      "Epoch 274/350\n",
      "75/75 [==============================] - 18s 237ms/step - loss: 2.0753e-04 - mean_absolute_error: 0.0116 - val_loss: 1.1369e-04 - val_mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.00010\n",
      "Epoch 275/350\n",
      "75/75 [==============================] - 17s 224ms/step - loss: 1.9559e-04 - mean_absolute_error: 0.0113 - val_loss: 9.8811e-05 - val_mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 00275: val_loss improved from 0.00010 to 0.00010, saving model to results\\2021-06-21_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "Epoch 276/350\n",
      "75/75 [==============================] - 17s 222ms/step - loss: 1.9303e-04 - mean_absolute_error: 0.0113 - val_loss: 1.2639e-04 - val_mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.00010\n",
      "Epoch 277/350\n",
      "75/75 [==============================] - 17s 225ms/step - loss: 2.0481e-04 - mean_absolute_error: 0.0116 - val_loss: 1.2064e-04 - val_mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.00010\n",
      "Epoch 278/350\n",
      "75/75 [==============================] - 17s 228ms/step - loss: 2.2283e-04 - mean_absolute_error: 0.0119 - val_loss: 1.1842e-04 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.00010\n",
      "Epoch 279/350\n",
      "75/75 [==============================] - 17s 232ms/step - loss: 2.0961e-04 - mean_absolute_error: 0.0117 - val_loss: 1.7282e-04 - val_mean_absolute_error: 0.0112\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.00010\n",
      "Epoch 280/350\n",
      "75/75 [==============================] - 17s 230ms/step - loss: 2.1749e-04 - mean_absolute_error: 0.0118 - val_loss: 1.1324e-04 - val_mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.00010\n",
      "Epoch 281/350\n",
      "75/75 [==============================] - 17s 225ms/step - loss: 1.9625e-04 - mean_absolute_error: 0.0115 - val_loss: 1.1271e-04 - val_mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.00010\n",
      "Epoch 282/350\n",
      "75/75 [==============================] - 17s 226ms/step - loss: 2.1760e-04 - mean_absolute_error: 0.0117 - val_loss: 1.0343e-04 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.00010\n",
      "Epoch 283/350\n",
      "75/75 [==============================] - 17s 228ms/step - loss: 1.8645e-04 - mean_absolute_error: 0.0113 - val_loss: 1.0405e-04 - val_mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.00010\n",
      "Epoch 284/350\n",
      "75/75 [==============================] - 17s 229ms/step - loss: 2.2128e-04 - mean_absolute_error: 0.0119 - val_loss: 1.0350e-04 - val_mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.00010\n",
      "Epoch 285/350\n",
      "75/75 [==============================] - 17s 233ms/step - loss: 1.9632e-04 - mean_absolute_error: 0.0112 - val_loss: 1.0770e-04 - val_mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.00010\n",
      "Epoch 286/350\n",
      "75/75 [==============================] - 17s 230ms/step - loss: 1.8908e-04 - mean_absolute_error: 0.0112 - val_loss: 1.4749e-04 - val_mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.00010\n",
      "Epoch 287/350\n",
      "75/75 [==============================] - 17s 227ms/step - loss: 2.0856e-04 - mean_absolute_error: 0.0115 - val_loss: 1.0113e-04 - val_mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.00010\n",
      "Epoch 288/350\n",
      "75/75 [==============================] - 17s 229ms/step - loss: 2.0851e-04 - mean_absolute_error: 0.0116 - val_loss: 1.5279e-04 - val_mean_absolute_error: 0.0107\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.00010\n",
      "Epoch 289/350\n",
      "75/75 [==============================] - 17s 225ms/step - loss: 1.9590e-04 - mean_absolute_error: 0.0113 - val_loss: 1.0261e-04 - val_mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.00010\n",
      "Epoch 290/350\n",
      "75/75 [==============================] - 17s 232ms/step - loss: 2.0251e-04 - mean_absolute_error: 0.0114 - val_loss: 1.2970e-04 - val_mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.00010\n",
      "Epoch 291/350\n",
      "75/75 [==============================] - 17s 227ms/step - loss: 2.1884e-04 - mean_absolute_error: 0.0118 - val_loss: 1.1043e-04 - val_mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.00010\n",
      "Epoch 292/350\n",
      "75/75 [==============================] - 17s 228ms/step - loss: 2.1321e-04 - mean_absolute_error: 0.0115 - val_loss: 1.0011e-04 - val_mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.00010\n",
      "Epoch 293/350\n",
      "75/75 [==============================] - 17s 227ms/step - loss: 2.0868e-04 - mean_absolute_error: 0.0115 - val_loss: 1.1611e-04 - val_mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.00010\n",
      "Epoch 294/350\n",
      "75/75 [==============================] - 17s 229ms/step - loss: 2.0124e-04 - mean_absolute_error: 0.0115 - val_loss: 1.0066e-04 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.00010\n",
      "Epoch 295/350\n",
      "75/75 [==============================] - 18s 237ms/step - loss: 2.0700e-04 - mean_absolute_error: 0.0115 - val_loss: 1.0154e-04 - val_mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.00010\n",
      "Epoch 296/350\n",
      "75/75 [==============================] - 17s 228ms/step - loss: 2.0684e-04 - mean_absolute_error: 0.0116 - val_loss: 1.0788e-04 - val_mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.00010\n",
      "Epoch 297/350\n",
      "75/75 [==============================] - 17s 225ms/step - loss: 2.0930e-04 - mean_absolute_error: 0.0115 - val_loss: 1.0077e-04 - val_mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.00010\n",
      "Epoch 298/350\n",
      "75/75 [==============================] - 17s 225ms/step - loss: 2.1093e-04 - mean_absolute_error: 0.0117 - val_loss: 1.2762e-04 - val_mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.00010\n",
      "Epoch 299/350\n",
      "75/75 [==============================] - 17s 226ms/step - loss: 2.0673e-04 - mean_absolute_error: 0.0115 - val_loss: 1.2059e-04 - val_mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.00010\n",
      "Epoch 300/350\n",
      "75/75 [==============================] - 17s 227ms/step - loss: 1.9347e-04 - mean_absolute_error: 0.0112 - val_loss: 1.1794e-04 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.00010\n",
      "Epoch 301/350\n",
      "75/75 [==============================] - 17s 225ms/step - loss: 2.1503e-04 - mean_absolute_error: 0.0117 - val_loss: 1.1649e-04 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.00010\n",
      "Epoch 302/350\n",
      "75/75 [==============================] - 17s 230ms/step - loss: 2.0579e-04 - mean_absolute_error: 0.0115 - val_loss: 1.0110e-04 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.00010\n",
      "Epoch 303/350\n",
      "75/75 [==============================] - 17s 229ms/step - loss: 1.8577e-04 - mean_absolute_error: 0.0110 - val_loss: 1.1066e-04 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 0.00010\n",
      "Epoch 304/350\n",
      "75/75 [==============================] - 17s 229ms/step - loss: 2.0148e-04 - mean_absolute_error: 0.0112 - val_loss: 9.6207e-05 - val_mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 00304: val_loss improved from 0.00010 to 0.00010, saving model to results\\2021-06-21_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "Epoch 305/350\n",
      "75/75 [==============================] - 18s 236ms/step - loss: 2.1809e-04 - mean_absolute_error: 0.0116 - val_loss: 1.1886e-04 - val_mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.00010\n",
      "Epoch 306/350\n",
      "75/75 [==============================] - 18s 234ms/step - loss: 1.9996e-04 - mean_absolute_error: 0.0113 - val_loss: 1.1671e-04 - val_mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 0.00010\n",
      "Epoch 307/350\n",
      "75/75 [==============================] - 17s 227ms/step - loss: 2.0124e-04 - mean_absolute_error: 0.0113 - val_loss: 1.3937e-04 - val_mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.00010\n",
      "Epoch 308/350\n",
      "75/75 [==============================] - 17s 227ms/step - loss: 2.1974e-04 - mean_absolute_error: 0.0118 - val_loss: 1.3908e-04 - val_mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.00010\n",
      "Epoch 309/350\n",
      "75/75 [==============================] - 17s 229ms/step - loss: 1.9629e-04 - mean_absolute_error: 0.0112 - val_loss: 1.0348e-04 - val_mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 0.00010\n",
      "Epoch 310/350\n",
      "75/75 [==============================] - 17s 231ms/step - loss: 1.9710e-04 - mean_absolute_error: 0.0114 - val_loss: 1.0524e-04 - val_mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.00010\n",
      "Epoch 311/350\n",
      "75/75 [==============================] - 17s 227ms/step - loss: 2.0562e-04 - mean_absolute_error: 0.0115 - val_loss: 1.1075e-04 - val_mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 0.00010\n",
      "Epoch 312/350\n",
      "75/75 [==============================] - 17s 228ms/step - loss: 1.9934e-04 - mean_absolute_error: 0.0114 - val_loss: 1.1951e-04 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 0.00010\n",
      "Epoch 313/350\n",
      "75/75 [==============================] - 18s 237ms/step - loss: 2.0093e-04 - mean_absolute_error: 0.0115 - val_loss: 1.1642e-04 - val_mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 0.00010\n",
      "Epoch 314/350\n",
      "75/75 [==============================] - 17s 230ms/step - loss: 1.9847e-04 - mean_absolute_error: 0.0113 - val_loss: 1.0381e-04 - val_mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 0.00010\n",
      "Epoch 315/350\n",
      "75/75 [==============================] - 17s 226ms/step - loss: 2.2787e-04 - mean_absolute_error: 0.0118 - val_loss: 1.4089e-04 - val_mean_absolute_error: 0.0096\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 0.00010\n",
      "Epoch 316/350\n",
      "75/75 [==============================] - 17s 224ms/step - loss: 2.0181e-04 - mean_absolute_error: 0.0114 - val_loss: 1.4656e-04 - val_mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 0.00010\n",
      "Epoch 317/350\n",
      "75/75 [==============================] - 17s 228ms/step - loss: 2.1003e-04 - mean_absolute_error: 0.0114 - val_loss: 9.7140e-05 - val_mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 0.00010\n",
      "Epoch 318/350\n",
      "75/75 [==============================] - 17s 227ms/step - loss: 2.0482e-04 - mean_absolute_error: 0.0114 - val_loss: 1.2422e-04 - val_mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 0.00010\n",
      "Epoch 319/350\n",
      "75/75 [==============================] - 17s 225ms/step - loss: 2.1028e-04 - mean_absolute_error: 0.0115 - val_loss: 1.0729e-04 - val_mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 0.00010\n",
      "Epoch 320/350\n",
      "75/75 [==============================] - 17s 228ms/step - loss: 1.9738e-04 - mean_absolute_error: 0.0113 - val_loss: 1.0830e-04 - val_mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 0.00010\n",
      "Epoch 321/350\n",
      "75/75 [==============================] - 17s 230ms/step - loss: 2.0877e-04 - mean_absolute_error: 0.0115 - val_loss: 1.1350e-04 - val_mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 0.00010\n",
      "Epoch 322/350\n",
      "75/75 [==============================] - 17s 228ms/step - loss: 2.0204e-04 - mean_absolute_error: 0.0115 - val_loss: 1.4374e-04 - val_mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 0.00010\n",
      "Epoch 323/350\n",
      "75/75 [==============================] - 17s 228ms/step - loss: 1.9959e-04 - mean_absolute_error: 0.0115 - val_loss: 1.0816e-04 - val_mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 0.00010\n",
      "Epoch 324/350\n",
      "75/75 [==============================] - 17s 231ms/step - loss: 2.0053e-04 - mean_absolute_error: 0.0114 - val_loss: 9.8370e-05 - val_mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 0.00010\n",
      "Epoch 325/350\n",
      "75/75 [==============================] - 17s 231ms/step - loss: 1.9133e-04 - mean_absolute_error: 0.0112 - val_loss: 1.0737e-04 - val_mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 0.00010\n",
      "Epoch 326/350\n",
      "75/75 [==============================] - 17s 230ms/step - loss: 1.9066e-04 - mean_absolute_error: 0.0112 - val_loss: 1.0958e-04 - val_mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 0.00010\n",
      "Epoch 327/350\n",
      "75/75 [==============================] - 17s 231ms/step - loss: 2.0801e-04 - mean_absolute_error: 0.0115 - val_loss: 9.9561e-05 - val_mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 0.00010\n",
      "Epoch 328/350\n",
      "75/75 [==============================] - 18s 246ms/step - loss: 1.9955e-04 - mean_absolute_error: 0.0113 - val_loss: 1.0269e-04 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 0.00010\n",
      "Epoch 329/350\n",
      "75/75 [==============================] - 17s 231ms/step - loss: 1.9647e-04 - mean_absolute_error: 0.0114 - val_loss: 1.0678e-04 - val_mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 0.00010\n",
      "Epoch 330/350\n",
      "75/75 [==============================] - 18s 240ms/step - loss: 2.0879e-04 - mean_absolute_error: 0.0115 - val_loss: 1.1403e-04 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 0.00010\n",
      "Epoch 331/350\n",
      "75/75 [==============================] - 17s 227ms/step - loss: 1.9657e-04 - mean_absolute_error: 0.0112 - val_loss: 1.0576e-04 - val_mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.00010\n",
      "Epoch 332/350\n",
      "75/75 [==============================] - 18s 234ms/step - loss: 2.1032e-04 - mean_absolute_error: 0.0115 - val_loss: 1.1869e-04 - val_mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 0.00010\n",
      "Epoch 333/350\n",
      "75/75 [==============================] - 17s 232ms/step - loss: 2.0749e-04 - mean_absolute_error: 0.0115 - val_loss: 9.9035e-05 - val_mean_absolute_error: 0.0070\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 0.00010\n",
      "Epoch 334/350\n",
      "75/75 [==============================] - 17s 222ms/step - loss: 2.1372e-04 - mean_absolute_error: 0.0115 - val_loss: 1.1627e-04 - val_mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 0.00010\n",
      "Epoch 335/350\n",
      "75/75 [==============================] - 17s 222ms/step - loss: 2.1117e-04 - mean_absolute_error: 0.0116 - val_loss: 1.1611e-04 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 0.00010\n",
      "Epoch 336/350\n",
      "75/75 [==============================] - 17s 223ms/step - loss: 2.0319e-04 - mean_absolute_error: 0.0116 - val_loss: 9.7390e-05 - val_mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 0.00010\n",
      "Epoch 337/350\n",
      "75/75 [==============================] - 17s 221ms/step - loss: 1.9229e-04 - mean_absolute_error: 0.0112 - val_loss: 1.3989e-04 - val_mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 0.00010\n",
      "Epoch 338/350\n",
      "75/75 [==============================] - 17s 220ms/step - loss: 1.8761e-04 - mean_absolute_error: 0.0112 - val_loss: 1.0638e-04 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 0.00010\n",
      "Epoch 339/350\n",
      "75/75 [==============================] - 19s 257ms/step - loss: 1.9756e-04 - mean_absolute_error: 0.0114 - val_loss: 1.1727e-04 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 0.00010\n",
      "Epoch 340/350\n",
      "75/75 [==============================] - 20s 269ms/step - loss: 2.2256e-04 - mean_absolute_error: 0.0118 - val_loss: 1.5444e-04 - val_mean_absolute_error: 0.0098\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 0.00010\n",
      "Epoch 341/350\n",
      "75/75 [==============================] - 18s 241ms/step - loss: 2.0682e-04 - mean_absolute_error: 0.0114 - val_loss: 1.0000e-04 - val_mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.00010\n",
      "Epoch 342/350\n",
      "75/75 [==============================] - 18s 235ms/step - loss: 1.8473e-04 - mean_absolute_error: 0.0110 - val_loss: 1.1887e-04 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.00010\n",
      "Epoch 343/350\n",
      "75/75 [==============================] - 17s 232ms/step - loss: 2.0068e-04 - mean_absolute_error: 0.0114 - val_loss: 1.0613e-04 - val_mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 0.00010\n",
      "Epoch 344/350\n",
      "75/75 [==============================] - 18s 237ms/step - loss: 1.9306e-04 - mean_absolute_error: 0.0111 - val_loss: 1.1085e-04 - val_mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 0.00010\n",
      "Epoch 345/350\n",
      "75/75 [==============================] - 18s 235ms/step - loss: 1.9573e-04 - mean_absolute_error: 0.0113 - val_loss: 1.5716e-04 - val_mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 0.00010\n",
      "Epoch 346/350\n",
      "75/75 [==============================] - 18s 235ms/step - loss: 2.0686e-04 - mean_absolute_error: 0.0115 - val_loss: 1.0123e-04 - val_mean_absolute_error: 0.0068\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.00010\n",
      "Epoch 347/350\n",
      "75/75 [==============================] - 17s 233ms/step - loss: 2.0284e-04 - mean_absolute_error: 0.0113 - val_loss: 1.1544e-04 - val_mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 0.00010\n",
      "Epoch 348/350\n",
      "75/75 [==============================] - 18s 235ms/step - loss: 2.1707e-04 - mean_absolute_error: 0.0116 - val_loss: 1.0788e-04 - val_mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 0.00010\n",
      "Epoch 349/350\n",
      "75/75 [==============================] - 18s 237ms/step - loss: 2.0773e-04 - mean_absolute_error: 0.0116 - val_loss: 1.3669e-04 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 0.00010\n",
      "Epoch 350/350\n",
      "75/75 [==============================] - 18s 239ms/step - loss: 1.8417e-04 - mean_absolute_error: 0.0110 - val_loss: 9.7331e-05 - val_mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 0.00010\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "data = load_data(ticker, N_STEPS, scale=SCALE, split_by_date=SPLIT_BY_DATE, \n",
    "                shuffle=SHUFFLE, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE, \n",
    "                feature_columns=FEATURE_COLUMNS)\n",
    "# save the dataframe\n",
    "data[\"df\"].to_csv(ticker_data_filename)\n",
    "# construct the model\n",
    "model = create_model(N_STEPS, len(FEATURE_COLUMNS), loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
    "                    dropout=DROPOUT, optimizer=OPTIMIZER, bidirectional=BIDIRECTIONAL)\n",
    "# some tensorflow callbacks\n",
    "checkpointer = ModelCheckpoint(os.path.join(\"results\", model_name + \".h5\"), save_weights_only=True, save_best_only=True, verbose=1)\n",
    "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
    "# train the model and save the weights whenever we see \n",
    "# a new optimal model using ModelCheckpoint\n",
    "history = model.fit(data[\"X_train\"], data[\"y_train\"],\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=(data[\"X_test\"], data[\"y_test\"]),\n",
    "                    callbacks=[checkpointer, tensorboard],\n",
    "                    verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Launching TensorBoard..."
     },
     "metadata": {}
    }
   ],
   "source": [
    "%tensorboard --logdir=\"logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graph(test_df):\n",
    "    \"\"\"\n",
    "    This function plots true close price along with predicted close price\n",
    "    with blue and red colors respectively\n",
    "    \"\"\"\n",
    "    plt.plot(test_df[f'true_adjclose_{LOOKUP_STEP}'], c='b')\n",
    "    plt.plot(test_df[f'adjclose_{LOOKUP_STEP}'], c='r')\n",
    "    plt.xlabel(\"Days\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.legend([\"Actual Price\", \"Predicted Price\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_df(model, data):\n",
    "    \"\"\"\n",
    "    This function takes the `model` and `data` dict to \n",
    "    construct a final dataframe that includes the features along \n",
    "    with true and predicted prices of the testing dataset\n",
    "    \"\"\"\n",
    "    # if predicted future price is higher than the current, \n",
    "    # then calculate the true future price minus the current price, to get the buy profit\n",
    "    buy_profit  = lambda current, pred_future, true_future: true_future - current if pred_future > current else 0\n",
    "    # if the predicted future price is lower than the current price,\n",
    "    # then subtract the true future price from the current price\n",
    "    sell_profit = lambda current, pred_future, true_future: current - true_future if pred_future < current else 0\n",
    "    X_test = data[\"X_test\"]\n",
    "    y_test = data[\"y_test\"]\n",
    "    # perform prediction and get prices\n",
    "    y_pred = model.predict(X_test)\n",
    "    if SCALE:\n",
    "        y_test = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(np.expand_dims(y_test, axis=0)))\n",
    "        y_pred = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(y_pred))\n",
    "    test_df = data[\"test_df\"]\n",
    "    # add predicted future prices to the dataframe\n",
    "    test_df[f\"adjclose_{LOOKUP_STEP}\"] = y_pred\n",
    "    # add true future prices to the dataframe\n",
    "    test_df[f\"true_adjclose_{LOOKUP_STEP}\"] = y_test\n",
    "    # sort the dataframe by date\n",
    "    test_df.sort_index(inplace=True)\n",
    "    final_df = test_df\n",
    "    # add the buy profit column\n",
    "    final_df[\"buy_profit\"] = list(map(buy_profit, \n",
    "                                    final_df[\"adjclose\"], \n",
    "                                    final_df[f\"adjclose_{LOOKUP_STEP}\"], \n",
    "                                    final_df[f\"true_adjclose_{LOOKUP_STEP}\"])\n",
    "                                    # since we don't have profit for last sequence, add 0's\n",
    "                                    )\n",
    "    # add the sell profit column\n",
    "    final_df[\"sell_profit\"] = list(map(sell_profit, \n",
    "                                    final_df[\"adjclose\"], \n",
    "                                    final_df[f\"adjclose_{LOOKUP_STEP}\"], \n",
    "                                    final_df[f\"true_adjclose_{LOOKUP_STEP}\"])\n",
    "                                    # since we don't have profit for last sequence, add 0's\n",
    "                                    )\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data):\n",
    "    # Extraer la última secuencia de nuestros datos\n",
    "    last_sequence = data[\"last_sequence\"][-N_STEPS:]\n",
    "\n",
    "    # Aumentar la dimension\n",
    "    last_sequence = np.expand_dims(last_sequence, axis=0)\n",
    "\n",
    "    # Obtener la prediccion en una escala de 0 a 1\n",
    "    prediction = model.predict(last_sequence)\n",
    "\n",
    "    # Obtener el precio (invirtiendo el escalado)\n",
    "    if SCALE:\n",
    "        predicted_price = data[\"column_scaler\"][\"adjclose\"].inverse_transform(prediction)[0][0]\n",
    "    else:\n",
    "        predicted_price = prediction[0][0]\n",
    "    return predicted_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ya se puede evaluar nuestro modelo\n",
    "# Para empezar, cargaremos los valores óptimos que se han guardado en la carpeta de resultados\n",
    "model_path = os.path.join(\"results\", model_name)+\".h5\"\n",
    "model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos el valor de loss y mean absolute error\n",
    "# Evaluamos el modelo\n",
    "loss, mae = model.evaluate(data[\"X_test\"], data[\"y_test\"], verbose=0)\n",
    "\n",
    "# Calculamos el mean absolute error (haciendo un escalado inverso)\n",
    "if SCALE:\n",
    "    mean_absolute_error = data[\"column_scaler\"][\"adjclose\"].inverse_transform([[mae]])[0][0]\n",
    "else:\n",
    "    mean_absolute_error = mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora utilizaremos la funcion get_final_df para obtener el conjunto de datos de test y poder hacer predicciones sobre el\n",
    "final_df = get_final_df(model, data)\n",
    "\n",
    "# Predicción de los precios futuros\n",
    "future_price = predict(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we calculate the accuracy by counting the number of positive profits\n",
    "accuracy_score = (len(final_df[final_df['sell_profit'] > 0]) + len(final_df[final_df['buy_profit'] > 0])) / len(final_df)\n",
    "# calculating total buy & sell profit\n",
    "total_buy_profit  = final_df[\"buy_profit\"].sum()\n",
    "total_sell_profit = final_df[\"sell_profit\"].sum()\n",
    "# total profit by adding sell & buy together\n",
    "total_profit = total_buy_profit + total_sell_profit\n",
    "# dividing total profit by number of testing samples (number of trades)\n",
    "profit_per_trade = total_profit / len(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Future price after 15 days is 3253.07$\nhuber_loss loss: 9.620676428312436e-05\nMean Absolute Error: 25.343246624034546\nAccuracy score: 0.5462114904246461\nTotal buy profit: 14020.716999053955\nTotal sell profit: 2068.9164395332327\nTotal profit: 16089.633438587189\nProfit per trade: 13.396863812312397\n\n\n                   open         high          low        close     adjclose  \\\n1997-08-06     2.208333     2.312500     2.187500     2.250000     2.250000   \n1997-08-07     2.250000     2.260417     2.125000     2.177083     2.177083   \n1997-08-18     2.052083     2.052083     1.968750     2.041667     2.041667   \n1997-08-19     2.093750     2.208333     2.052083     2.166667     2.166667   \n1997-08-21     2.135417     2.171875     2.072917     2.114583     2.114583   \n...                 ...          ...          ...          ...          ...   \n2021-04-28  3434.800049  3489.879883  3425.000000  3458.500000  3458.500000   \n2021-04-30  3525.120117  3554.000000  3462.500000  3467.419922  3467.419922   \n2021-05-06  3270.000000  3314.399902  3247.199951  3306.370117  3306.370117   \n2021-05-17  3245.929932  3292.750000  3234.590088  3270.389893  3270.389893   \n2021-05-24  3215.500000  3257.949951  3210.500000  3244.989990  3244.989990   \n\n             volume ticker  adjclose_15  true_adjclose_15  buy_profit  \\\n1997-08-06  1243200   AMZN     3.437103          2.317708    0.067708   \n1997-08-07  2034000   AMZN     3.162731          2.375000    0.197917   \n1997-08-18  1784400   AMZN     3.654296          3.239583    1.197916   \n1997-08-19  1003200   AMZN     3.761130          3.302083    1.135416   \n1997-08-21   624000   AMZN     3.140218          3.687500    1.572917   \n...             ...    ...          ...               ...         ...   \n2021-04-28  4631900   AMZN  3254.963379       3231.800049    0.000000   \n2021-04-30  7009300   AMZN  3252.952393       3203.080078    0.000000   \n2021-05-06  4447700   AMZN  3259.049561       3230.110107    0.000000   \n2021-05-17  3723900   AMZN  3258.331055       3264.110107    0.000000   \n2021-05-24  2422800   AMZN  3258.392578       3383.129883  138.139893   \n\n            sell_profit  \n1997-08-06     0.000000  \n1997-08-07     0.000000  \n1997-08-18     0.000000  \n1997-08-19     0.000000  \n1997-08-21     0.000000  \n...                 ...  \n2021-04-28   226.699951  \n2021-04-30   264.339844  \n2021-05-06    76.260010  \n2021-05-17     6.279785  \n2021-05-24     0.000000  \n\n[1201 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "# Imprimimos los resultados\n",
    "\n",
    "print(f\"Future price after {LOOKUP_STEP} days is {future_price:.2f}$\")\n",
    "print(f\"{LOSS} loss:\", loss)\n",
    "print(\"Mean Absolute Error:\", mean_absolute_error)\n",
    "print(\"Accuracy score:\", accuracy_score)\n",
    "print(\"Total buy profit:\", total_buy_profit)\n",
    "print(\"Total sell profit:\", total_sell_profit)\n",
    "print(\"Total profit:\", total_profit)\n",
    "print(\"Profit per trade:\", profit_per_trade)\n",
    "print(\"\\n\")\n",
    "print(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<svg height=\"262.19625pt\" version=\"1.1\" viewBox=\"0 0 395.328125 262.19625\" width=\"395.328125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-06-21T20:53:56.922830</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.4.2, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 262.19625 \r\nL 395.328125 262.19625 \r\nL 395.328125 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 53.328125 224.64 \r\nL 388.128125 224.64 \r\nL 388.128125 7.2 \r\nL 53.328125 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m5afa6f45a8\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"99.290816\" xlink:href=\"#m5afa6f45a8\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 2000 -->\r\n      <g transform=\"translate(86.565816 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 1228 531 \r\nL 3431 531 \r\nL 3431 0 \r\nL 469 0 \r\nL 469 531 \r\nQ 828 903 1448 1529 \r\nQ 2069 2156 2228 2338 \r\nQ 2531 2678 2651 2914 \r\nQ 2772 3150 2772 3378 \r\nQ 2772 3750 2511 3984 \r\nQ 2250 4219 1831 4219 \r\nQ 1534 4219 1204 4116 \r\nQ 875 4013 500 3803 \r\nL 500 4441 \r\nQ 881 4594 1212 4672 \r\nQ 1544 4750 1819 4750 \r\nQ 2544 4750 2975 4387 \r\nQ 3406 4025 3406 3419 \r\nQ 3406 3131 3298 2873 \r\nQ 3191 2616 2906 2266 \r\nQ 2828 2175 2409 1742 \r\nQ 1991 1309 1228 531 \r\nz\r\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\r\n        <path d=\"M 2034 4250 \r\nQ 1547 4250 1301 3770 \r\nQ 1056 3291 1056 2328 \r\nQ 1056 1369 1301 889 \r\nQ 1547 409 2034 409 \r\nQ 2525 409 2770 889 \r\nQ 3016 1369 3016 2328 \r\nQ 3016 3291 2770 3770 \r\nQ 2525 4250 2034 4250 \r\nz\r\nM 2034 4750 \r\nQ 2819 4750 3233 4129 \r\nQ 3647 3509 3647 2328 \r\nQ 3647 1150 3233 529 \r\nQ 2819 -91 2034 -91 \r\nQ 1250 -91 836 529 \r\nQ 422 1150 422 2328 \r\nQ 422 3509 836 4129 \r\nQ 1250 4750 2034 4750 \r\nz\r\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"150.449959\" xlink:href=\"#m5afa6f45a8\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 2004 -->\r\n      <g transform=\"translate(137.724959 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2419 4116 \r\nL 825 1625 \r\nL 2419 1625 \r\nL 2419 4116 \r\nz\r\nM 2253 4666 \r\nL 3047 4666 \r\nL 3047 1625 \r\nL 3713 1625 \r\nL 3713 1100 \r\nL 3047 1100 \r\nL 3047 0 \r\nL 2419 0 \r\nL 2419 1100 \r\nL 313 1100 \r\nL 313 1709 \r\nL 2253 4666 \r\nz\r\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-34\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"201.609102\" xlink:href=\"#m5afa6f45a8\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 2008 -->\r\n      <g transform=\"translate(188.884102 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2034 2216 \r\nQ 1584 2216 1326 1975 \r\nQ 1069 1734 1069 1313 \r\nQ 1069 891 1326 650 \r\nQ 1584 409 2034 409 \r\nQ 2484 409 2743 651 \r\nQ 3003 894 3003 1313 \r\nQ 3003 1734 2745 1975 \r\nQ 2488 2216 2034 2216 \r\nz\r\nM 1403 2484 \r\nQ 997 2584 770 2862 \r\nQ 544 3141 544 3541 \r\nQ 544 4100 942 4425 \r\nQ 1341 4750 2034 4750 \r\nQ 2731 4750 3128 4425 \r\nQ 3525 4100 3525 3541 \r\nQ 3525 3141 3298 2862 \r\nQ 3072 2584 2669 2484 \r\nQ 3125 2378 3379 2068 \r\nQ 3634 1759 3634 1313 \r\nQ 3634 634 3220 271 \r\nQ 2806 -91 2034 -91 \r\nQ 1263 -91 848 271 \r\nQ 434 634 434 1313 \r\nQ 434 1759 690 2068 \r\nQ 947 2378 1403 2484 \r\nz\r\nM 1172 3481 \r\nQ 1172 3119 1398 2916 \r\nQ 1625 2713 2034 2713 \r\nQ 2441 2713 2670 2916 \r\nQ 2900 3119 2900 3481 \r\nQ 2900 3844 2670 4047 \r\nQ 2441 4250 2034 4250 \r\nQ 1625 4250 1398 4047 \r\nQ 1172 3844 1172 3481 \r\nz\r\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-38\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"252.768245\" xlink:href=\"#m5afa6f45a8\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 2012 -->\r\n      <g transform=\"translate(240.043245 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 794 531 \r\nL 1825 531 \r\nL 1825 4091 \r\nL 703 3866 \r\nL 703 4441 \r\nL 1819 4666 \r\nL 2450 4666 \r\nL 2450 531 \r\nL 3481 531 \r\nL 3481 0 \r\nL 794 0 \r\nL 794 531 \r\nz\r\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-32\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"303.927389\" xlink:href=\"#m5afa6f45a8\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 2016 -->\r\n      <g transform=\"translate(291.202389 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2113 2584 \r\nQ 1688 2584 1439 2293 \r\nQ 1191 2003 1191 1497 \r\nQ 1191 994 1439 701 \r\nQ 1688 409 2113 409 \r\nQ 2538 409 2786 701 \r\nQ 3034 994 3034 1497 \r\nQ 3034 2003 2786 2293 \r\nQ 2538 2584 2113 2584 \r\nz\r\nM 3366 4563 \r\nL 3366 3988 \r\nQ 3128 4100 2886 4159 \r\nQ 2644 4219 2406 4219 \r\nQ 1781 4219 1451 3797 \r\nQ 1122 3375 1075 2522 \r\nQ 1259 2794 1537 2939 \r\nQ 1816 3084 2150 3084 \r\nQ 2853 3084 3261 2657 \r\nQ 3669 2231 3669 1497 \r\nQ 3669 778 3244 343 \r\nQ 2819 -91 2113 -91 \r\nQ 1303 -91 875 529 \r\nQ 447 1150 447 2328 \r\nQ 447 3434 972 4092 \r\nQ 1497 4750 2381 4750 \r\nQ 2619 4750 2861 4703 \r\nQ 3103 4656 3366 4563 \r\nz\r\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-36\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"355.086532\" xlink:href=\"#m5afa6f45a8\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 2020 -->\r\n      <g transform=\"translate(342.361532 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_7\">\r\n     <!-- Days -->\r\n     <g transform=\"translate(208.25 252.916562)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 1259 4147 \r\nL 1259 519 \r\nL 2022 519 \r\nQ 2988 519 3436 956 \r\nQ 3884 1394 3884 2338 \r\nQ 3884 3275 3436 3711 \r\nQ 2988 4147 2022 4147 \r\nL 1259 4147 \r\nz\r\nM 628 4666 \r\nL 1925 4666 \r\nQ 3281 4666 3915 4102 \r\nQ 4550 3538 4550 2338 \r\nQ 4550 1131 3912 565 \r\nQ 3275 0 1925 0 \r\nL 628 0 \r\nL 628 4666 \r\nz\r\n\" id=\"DejaVuSans-44\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 2194 1759 \r\nQ 1497 1759 1228 1600 \r\nQ 959 1441 959 1056 \r\nQ 959 750 1161 570 \r\nQ 1363 391 1709 391 \r\nQ 2188 391 2477 730 \r\nQ 2766 1069 2766 1631 \r\nL 2766 1759 \r\nL 2194 1759 \r\nz\r\nM 3341 1997 \r\nL 3341 0 \r\nL 2766 0 \r\nL 2766 531 \r\nQ 2569 213 2275 61 \r\nQ 1981 -91 1556 -91 \r\nQ 1019 -91 701 211 \r\nQ 384 513 384 1019 \r\nQ 384 1609 779 1909 \r\nQ 1175 2209 1959 2209 \r\nL 2766 2209 \r\nL 2766 2266 \r\nQ 2766 2663 2505 2880 \r\nQ 2244 3097 1772 3097 \r\nQ 1472 3097 1187 3025 \r\nQ 903 2953 641 2809 \r\nL 641 3341 \r\nQ 956 3463 1253 3523 \r\nQ 1550 3584 1831 3584 \r\nQ 2591 3584 2966 3190 \r\nQ 3341 2797 3341 1997 \r\nz\r\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 2059 -325 \r\nQ 1816 -950 1584 -1140 \r\nQ 1353 -1331 966 -1331 \r\nL 506 -1331 \r\nL 506 -850 \r\nL 844 -850 \r\nQ 1081 -850 1212 -737 \r\nQ 1344 -625 1503 -206 \r\nL 1606 56 \r\nL 191 3500 \r\nL 800 3500 \r\nL 1894 763 \r\nL 2988 3500 \r\nL 3597 3500 \r\nL 2059 -325 \r\nz\r\n\" id=\"DejaVuSans-79\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 2834 3397 \r\nL 2834 2853 \r\nQ 2591 2978 2328 3040 \r\nQ 2066 3103 1784 3103 \r\nQ 1356 3103 1142 2972 \r\nQ 928 2841 928 2578 \r\nQ 928 2378 1081 2264 \r\nQ 1234 2150 1697 2047 \r\nL 1894 2003 \r\nQ 2506 1872 2764 1633 \r\nQ 3022 1394 3022 966 \r\nQ 3022 478 2636 193 \r\nQ 2250 -91 1575 -91 \r\nQ 1294 -91 989 -36 \r\nQ 684 19 347 128 \r\nL 347 722 \r\nQ 666 556 975 473 \r\nQ 1284 391 1588 391 \r\nQ 1994 391 2212 530 \r\nQ 2431 669 2431 922 \r\nQ 2431 1156 2273 1281 \r\nQ 2116 1406 1581 1522 \r\nL 1381 1569 \r\nQ 847 1681 609 1914 \r\nQ 372 2147 372 2553 \r\nQ 372 3047 722 3315 \r\nQ 1072 3584 1716 3584 \r\nQ 2034 3584 2315 3537 \r\nQ 2597 3491 2834 3397 \r\nz\r\n\" id=\"DejaVuSans-73\" transform=\"scale(0.015625)\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-44\"/>\r\n      <use x=\"77.001953\" xlink:href=\"#DejaVuSans-61\"/>\r\n      <use x=\"138.28125\" xlink:href=\"#DejaVuSans-79\"/>\r\n      <use x=\"197.460938\" xlink:href=\"#DejaVuSans-73\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m19515141aa\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"53.328125\" xlink:href=\"#m19515141aa\" y=\"214.888433\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(39.965625 218.687652)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"53.328125\" xlink:href=\"#m19515141aa\" y=\"186.397058\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 500 -->\r\n      <g transform=\"translate(27.240625 190.196276)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 691 4666 \r\nL 3169 4666 \r\nL 3169 4134 \r\nL 1269 4134 \r\nL 1269 2991 \r\nQ 1406 3038 1543 3061 \r\nQ 1681 3084 1819 3084 \r\nQ 2600 3084 3056 2656 \r\nQ 3513 2228 3513 1497 \r\nQ 3513 744 3044 326 \r\nQ 2575 -91 1722 -91 \r\nQ 1428 -91 1123 -41 \r\nQ 819 9 494 109 \r\nL 494 744 \r\nQ 775 591 1075 516 \r\nQ 1375 441 1709 441 \r\nQ 2250 441 2565 725 \r\nQ 2881 1009 2881 1497 \r\nQ 2881 1984 2565 2268 \r\nQ 2250 2553 1709 2553 \r\nQ 1456 2553 1204 2497 \r\nQ 953 2441 691 2322 \r\nL 691 4666 \r\nz\r\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"53.328125\" xlink:href=\"#m19515141aa\" y=\"157.905682\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 1000 -->\r\n      <g transform=\"translate(20.878125 161.704901)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"53.328125\" xlink:href=\"#m19515141aa\" y=\"129.414307\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 1500 -->\r\n      <g transform=\"translate(20.878125 133.213525)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"53.328125\" xlink:href=\"#m19515141aa\" y=\"100.922931\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 2000 -->\r\n      <g transform=\"translate(20.878125 104.72215)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"53.328125\" xlink:href=\"#m19515141aa\" y=\"72.431556\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 2500 -->\r\n      <g transform=\"translate(20.878125 76.230774)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"53.328125\" xlink:href=\"#m19515141aa\" y=\"43.94018\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 3000 -->\r\n      <g transform=\"translate(20.878125 47.739399)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2597 2516 \r\nQ 3050 2419 3304 2112 \r\nQ 3559 1806 3559 1356 \r\nQ 3559 666 3084 287 \r\nQ 2609 -91 1734 -91 \r\nQ 1441 -91 1130 -33 \r\nQ 819 25 488 141 \r\nL 488 750 \r\nQ 750 597 1062 519 \r\nQ 1375 441 1716 441 \r\nQ 2309 441 2620 675 \r\nQ 2931 909 2931 1356 \r\nQ 2931 1769 2642 2001 \r\nQ 2353 2234 1838 2234 \r\nL 1294 2234 \r\nL 1294 2753 \r\nL 1863 2753 \r\nQ 2328 2753 2575 2939 \r\nQ 2822 3125 2822 3475 \r\nQ 2822 3834 2567 4026 \r\nQ 2313 4219 1838 4219 \r\nQ 1578 4219 1281 4162 \r\nQ 984 4106 628 3988 \r\nL 628 4550 \r\nQ 988 4650 1302 4700 \r\nQ 1616 4750 1894 4750 \r\nQ 2613 4750 3031 4423 \r\nQ 3450 4097 3450 3541 \r\nQ 3450 3153 3228 2886 \r\nQ 3006 2619 2597 2516 \r\nz\r\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-33\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_8\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"53.328125\" xlink:href=\"#m19515141aa\" y=\"15.448805\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_15\">\r\n      <!-- 3500 -->\r\n      <g transform=\"translate(20.878125 19.248023)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-33\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_16\">\r\n     <!-- Price -->\r\n     <g transform=\"translate(14.798438 128.117656)rotate(-90)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 1259 4147 \r\nL 1259 2394 \r\nL 2053 2394 \r\nQ 2494 2394 2734 2622 \r\nQ 2975 2850 2975 3272 \r\nQ 2975 3691 2734 3919 \r\nQ 2494 4147 2053 4147 \r\nL 1259 4147 \r\nz\r\nM 628 4666 \r\nL 2053 4666 \r\nQ 2838 4666 3239 4311 \r\nQ 3641 3956 3641 3272 \r\nQ 3641 2581 3239 2228 \r\nQ 2838 1875 2053 1875 \r\nL 1259 1875 \r\nL 1259 0 \r\nL 628 0 \r\nL 628 4666 \r\nz\r\n\" id=\"DejaVuSans-50\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 2631 2963 \r\nQ 2534 3019 2420 3045 \r\nQ 2306 3072 2169 3072 \r\nQ 1681 3072 1420 2755 \r\nQ 1159 2438 1159 1844 \r\nL 1159 0 \r\nL 581 0 \r\nL 581 3500 \r\nL 1159 3500 \r\nL 1159 2956 \r\nQ 1341 3275 1631 3429 \r\nQ 1922 3584 2338 3584 \r\nQ 2397 3584 2469 3576 \r\nQ 2541 3569 2628 3553 \r\nL 2631 2963 \r\nz\r\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 603 3500 \r\nL 1178 3500 \r\nL 1178 0 \r\nL 603 0 \r\nL 603 3500 \r\nz\r\nM 603 4863 \r\nL 1178 4863 \r\nL 1178 4134 \r\nL 603 4134 \r\nL 603 4863 \r\nz\r\n\" id=\"DejaVuSans-69\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 3122 3366 \r\nL 3122 2828 \r\nQ 2878 2963 2633 3030 \r\nQ 2388 3097 2138 3097 \r\nQ 1578 3097 1268 2742 \r\nQ 959 2388 959 1747 \r\nQ 959 1106 1268 751 \r\nQ 1578 397 2138 397 \r\nQ 2388 397 2633 464 \r\nQ 2878 531 3122 666 \r\nL 3122 134 \r\nQ 2881 22 2623 -34 \r\nQ 2366 -91 2075 -91 \r\nQ 1284 -91 818 406 \r\nQ 353 903 353 1747 \r\nQ 353 2603 823 3093 \r\nQ 1294 3584 2113 3584 \r\nQ 2378 3584 2631 3529 \r\nQ 2884 3475 3122 3366 \r\nz\r\n\" id=\"DejaVuSans-63\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 3597 1894 \r\nL 3597 1613 \r\nL 953 1613 \r\nQ 991 1019 1311 708 \r\nQ 1631 397 2203 397 \r\nQ 2534 397 2845 478 \r\nQ 3156 559 3463 722 \r\nL 3463 178 \r\nQ 3153 47 2828 -22 \r\nQ 2503 -91 2169 -91 \r\nQ 1331 -91 842 396 \r\nQ 353 884 353 1716 \r\nQ 353 2575 817 3079 \r\nQ 1281 3584 2069 3584 \r\nQ 2775 3584 3186 3129 \r\nQ 3597 2675 3597 1894 \r\nz\r\nM 3022 2063 \r\nQ 3016 2534 2758 2815 \r\nQ 2500 3097 2075 3097 \r\nQ 1594 3097 1305 2825 \r\nQ 1016 2553 972 2059 \r\nL 3022 2063 \r\nz\r\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-50\"/>\r\n      <use x=\"58.552734\" xlink:href=\"#DejaVuSans-72\"/>\r\n      <use x=\"99.666016\" xlink:href=\"#DejaVuSans-69\"/>\r\n      <use x=\"127.449219\" xlink:href=\"#DejaVuSans-63\"/>\r\n      <use x=\"182.429688\" xlink:href=\"#DejaVuSans-65\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_15\">\r\n    <path clip-path=\"url(#p7ba2364ce6)\" d=\"M 68.546307 214.756364 \r\nL 70.682315 214.645069 \r\nL 78.59605 214.294863 \r\nL 79.261364 213.950592 \r\nL 79.40143 213.563584 \r\nL 79.611529 213.943469 \r\nL 80.10176 213.708415 \r\nL 80.872124 213.722661 \r\nL 81.362355 213.882776 \r\nL 81.852586 214.188614 \r\nL 83.043148 213.966618 \r\nL 84.303743 213.173015 \r\nL 85.14414 212.583006 \r\nL 85.214173 212.260104 \r\nL 85.459288 211.801867 \r\nL 85.669388 211.837481 \r\nL 85.774437 211.341257 \r\nL 85.984536 209.628213 \r\nL 86.64985 211.309204 \r\nL 87.45523 211.985874 \r\nL 87.630313 211.727671 \r\nL 87.980478 211.407143 \r\nL 88.47071 211.040317 \r\nL 88.890908 209.982574 \r\nL 89.206057 209.633555 \r\nL 89.556222 209.987916 \r\nL 89.941404 209.986136 \r\nL 90.851833 211.442757 \r\nL 91.342065 211.585214 \r\nL 91.412098 212.267226 \r\nL 91.58718 211.670688 \r\nL 91.622197 211.716987 \r\nL 91.832296 211.533574 \r\nL 91.902329 211.74904 \r\nL 92.322527 211.316327 \r\nL 93.583122 212.274349 \r\nL 93.793221 211.665346 \r\nL 93.968304 211.515766 \r\nL 94.073354 211.225511 \r\nL 94.318469 211.465907 \r\nL 94.738668 211.152501 \r\nL 94.948767 211.341257 \r\nL 95.333949 210.486516 \r\nL 95.999263 210.301322 \r\nL 96.069296 210.408164 \r\nL 96.174345 210.17311 \r\nL 96.209362 210.258584 \r\nL 96.559527 211.188116 \r\nL 96.73461 210.785675 \r\nL 97.53999 209.888197 \r\nL 97.785105 209.047701 \r\nL 98.135271 209.361106 \r\nL 98.625502 210.219409 \r\nL 98.975667 211.227291 \r\nL 99.360849 210.942378 \r\nL 99.711014 210.411726 \r\nL 99.746031 210.614727 \r\nL 99.991146 210.54706 \r\nL 101.146692 211.255783 \r\nL 101.181708 211.113326 \r\nL 101.56689 210.721569 \r\nL 101.952072 211.038536 \r\nL 102.197187 212.217367 \r\nL 102.547353 211.9004 \r\nL 102.652402 211.743697 \r\nL 103.352733 211.412485 \r\nL 103.387749 211.733013 \r\nL 103.527815 211.932453 \r\nL 103.772931 211.939576 \r\nL 104.543295 212.260104 \r\nL 104.823427 212.730211 \r\nL 106.329137 213.154021 \r\nL 106.50422 212.748018 \r\nL 107.484683 212.470228 \r\nL 107.799831 212.52721 \r\nL 108.535179 213.456741 \r\nL 108.815311 213.132652 \r\nL 109.305542 212.748018 \r\nL 110.215972 213.481671 \r\nL 110.776236 213.584953 \r\nL 111.231451 214.001639 \r\nL 111.686666 213.880551 \r\nL 112.702145 214.069306 \r\nL 114.698087 214.411487 \r\nL 115.188318 213.965312 \r\nL 115.398417 214.018306 \r\nL 115.643533 213.887816 \r\nL 115.853632 214.033122 \r\nL 116.764062 213.951637 \r\nL 117.149244 214.214327 \r\nL 118.339806 213.920866 \r\nL 118.514888 214.232562 \r\nL 119.075153 214.292393 \r\nL 120.580863 214.548246 \r\nL 125.168028 214.079848 \r\nL 125.973408 214.10606 \r\nL 126.393606 213.998362 \r\nL 128.0744 214.087256 \r\nL 128.354532 213.948787 \r\nL 129.054862 213.819437 \r\nL 130.245424 213.890665 \r\nL 130.315457 213.985256 \r\nL 132.696581 214.056485 \r\nL 132.941697 213.912319 \r\nL 133.046746 213.941949 \r\nL 135.217771 213.725415 \r\nL 135.392853 213.61088 \r\nL 135.42787 213.67527 \r\nL 138.159159 213.629684 \r\nL 138.439291 213.745359 \r\nL 140.120084 213.385228 \r\nL 140.575299 213.456457 \r\nL 142.256092 212.880931 \r\nL 143.096489 212.796026 \r\nL 143.481671 212.580062 \r\nL 143.796819 212.81882 \r\nL 144.252034 212.529917 \r\nL 144.602199 212.657558 \r\nL 145.442596 212.191439 \r\nL 145.967844 212.258109 \r\nL 146.528108 212.034167 \r\nL 147.01834 211.474596 \r\nL 147.683654 211.648394 \r\nL 148.138868 212.018782 \r\nL 148.383984 211.963508 \r\nL 148.6291 211.815353 \r\nL 148.664116 211.953252 \r\nL 148.874215 212.076904 \r\nL 149.644579 211.890001 \r\nL 149.959728 211.871196 \r\nL 150.13481 211.687142 \r\nL 150.34491 211.686002 \r\nL 152.165769 212.536185 \r\nL 153.181248 212.15554 \r\nL 153.391347 212.295718 \r\nL 154.546893 212.495727 \r\nL 155.247223 211.928749 \r\nL 155.352273 212.082033 \r\nL 155.387289 212.033027 \r\nL 155.772471 211.993139 \r\nL 156.052603 211.89342 \r\nL 157.348215 212.866115 \r\nL 157.838446 212.637045 \r\nL 158.013529 212.592028 \r\nL 158.538776 212.690608 \r\nL 158.713859 212.483191 \r\nL 160.00947 212.639324 \r\nL 160.044487 212.91683 \r\nL 160.709801 212.71796 \r\nL 160.779834 212.582911 \r\nL 162.180495 212.636475 \r\nL 162.600693 212.48718 \r\nL 163.896305 212.843322 \r\nL 165.296966 212.969824 \r\nL 165.927263 212.91683 \r\nL 166.277428 213.008002 \r\nL 168.27337 212.880361 \r\nL 169.113767 212.860417 \r\nL 170.479411 212.365807 \r\nL 171.004659 212.45527 \r\nL 172.195221 212.328198 \r\nL 173.140667 212.617101 \r\nL 173.770965 212.154401 \r\nL 174.891494 212.145283 \r\nL 176.117072 212.403985 \r\nL 176.187105 212.311673 \r\nL 176.922452 212.657558 \r\nL 178.183047 212.867255 \r\nL 178.638262 212.694027 \r\nL 180.003906 213.020539 \r\nL 180.389088 212.833065 \r\nL 180.879319 212.973813 \r\nL 181.719716 212.688329 \r\nL 181.999848 212.91512 \r\nL 182.350013 212.933355 \r\nL 182.735195 213.388647 \r\nL 184.065823 213.133934 \r\nL 184.731137 213.046181 \r\nL 184.90622 213.129375 \r\nL 185.501501 213.034214 \r\nL 185.7116 212.741323 \r\nL 186.236848 212.675223 \r\nL 186.446947 212.460968 \r\nL 187.497443 212.608553 \r\nL 187.742558 212.595447 \r\nL 188.93312 212.788619 \r\nL 189.388335 212.667245 \r\nL 191.279227 212.58519 \r\nL 191.55936 212.513392 \r\nL 191.769459 212.32022 \r\nL 192.014574 212.338455 \r\nL 192.119624 211.321313 \r\nL 192.819954 211.285983 \r\nL 192.995037 210.996511 \r\nL 193.485268 210.691653 \r\nL 193.835434 210.762882 \r\nL 194.220615 210.975997 \r\nL 195.025996 210.708748 \r\nL 195.306128 210.101312 \r\nL 195.516227 210.483097 \r\nL 195.761343 210.456315 \r\nL 195.796359 210.665441 \r\nL 196.041475 210.740659 \r\nL 196.216557 210.47284 \r\nL 196.426657 210.40674 \r\nL 197.547185 209.58049 \r\nL 197.967384 209.494446 \r\nL 198.212499 209.728645 \r\nL 198.282532 209.773661 \r\nL 199.122929 210.500761 \r\nL 199.192962 210.414717 \r\nL 199.262995 210.409589 \r\nL 199.928309 209.520088 \r\nL 200.138408 209.687048 \r\nL 200.873755 209.463105 \r\nL 201.223921 210.316137 \r\nL 201.924251 210.63581 \r\nL 202.204383 210.6039 \r\nL 203.184846 211.271738 \r\nL 204.270358 210.483666 \r\nL 204.76059 210.319556 \r\nL 204.865639 210.461443 \r\nL 205.145771 210.483097 \r\nL 205.390887 210.762312 \r\nL 206.336333 210.293914 \r\nL 206.826565 210.087066 \r\nL 207.07168 210.540649 \r\nL 207.456862 210.612447 \r\nL 207.701978 211.068879 \r\nL 207.736994 210.794792 \r\nL 207.807027 210.949785 \r\nL 207.98211 210.86887 \r\nL 208.78749 209.872242 \r\nL 208.962573 210.256305 \r\nL 209.277721 210.134932 \r\nL 210.293201 210.859753 \r\nL 210.923498 211.705377 \r\nL 210.993531 212.02277 \r\nL 211.413729 211.69512 \r\nL 211.763895 212.084312 \r\nL 211.973994 212.328198 \r\nL 212.184093 212.846171 \r\nL 212.849407 211.968067 \r\nL 212.91944 212.139015 \r\nL 212.954456 211.968067 \r\nL 213.094523 211.889431 \r\nL 214.565217 212.128189 \r\nL 214.705283 211.403938 \r\nL 215.090464 211.280855 \r\nL 215.370597 211.358352 \r\nL 216.421092 210.90192 \r\nL 216.666208 210.87001 \r\nL 216.911324 210.434091 \r\nL 217.05139 210.585665 \r\nL 217.121423 210.342919 \r\nL 217.331522 210.486516 \r\nL 217.366539 210.440929 \r\nL 218.031853 210.221546 \r\nL 218.627133 210.578258 \r\nL 219.327464 210.097323 \r\nL 219.747662 210.462013 \r\nL 219.852712 210.10872 \r\nL 220.062811 210.368561 \r\nL 220.798158 209.959995 \r\nL 221.568522 210.125815 \r\nL 221.743604 210.158865 \r\nL 221.918687 210.073391 \r\nL 222.023736 210.084217 \r\nL 222.7941 209.728075 \r\nL 222.89915 209.5463 \r\nL 223.389381 209.835773 \r\nL 223.49443 209.462535 \r\nL 223.774563 209.456837 \r\nL 224.229778 207.903487 \r\nL 224.439877 208.215753 \r\nL 224.474893 208.015743 \r\nL 224.720009 207.450475 \r\nL 225.140207 207.313146 \r\nL 225.385323 206.782637 \r\nL 225.595422 207.246476 \r\nL 225.980604 207.567289 \r\nL 226.365786 207.110857 \r\nL 226.400802 207.223113 \r\nL 226.645918 207.280096 \r\nL 226.715951 207.63168 \r\nL 226.821 207.53139 \r\nL 227.346248 207.8938 \r\nL 227.416281 207.742226 \r\nL 228.571827 207.714875 \r\nL 229.132091 207.323403 \r\nL 229.307174 207.522843 \r\nL 229.377207 207.211147 \r\nL 229.587306 207.151885 \r\nL 229.657339 207.395771 \r\nL 229.832422 206.856144 \r\nL 230.007504 206.90173 \r\nL 230.042521 206.666962 \r\nL 230.357669 206.704 \r\nL 230.497736 206.5057 \r\nL 231.373149 207.89551 \r\nL 232.318595 207.718293 \r\nL 232.353611 207.905197 \r\nL 232.948892 208.616912 \r\nL 233.053942 208.206636 \r\nL 233.334074 208.053922 \r\nL 233.824305 208.170737 \r\nL 234.209487 207.480675 \r\nL 234.314537 207.783254 \r\nL 234.979851 207.338788 \r\nL 235.750214 205.729596 \r\nL 235.99533 205.938722 \r\nL 236.450545 206.04642 \r\nL 237.150875 205.508502 \r\nL 237.746156 205.496536 \r\nL 237.956255 204.79109 \r\nL 238.341437 204.815592 \r\nL 238.866685 204.36087 \r\nL 239.356916 204.208726 \r\nL 240.12728 204.377965 \r\nL 240.197313 205.221879 \r\nL 240.442429 204.834967 \r\nL 240.792594 204.037778 \r\nL 240.82761 204.116984 \r\nL 240.897643 204.189352 \r\nL 241.03771 204.607605 \r\nL 241.878106 205.71592 \r\nL 242.298304 204.938105 \r\nL 242.613453 204.351752 \r\nL 242.753519 204.401328 \r\nL 242.788536 204.604186 \r\nL 242.823552 204.501048 \r\nL 243.243751 204.500477 \r\nL 243.313784 203.772807 \r\nL 243.523883 203.493592 \r\nL 244.994577 204.290781 \r\nL 245.239693 203.820674 \r\nL 245.344742 203.916404 \r\nL 245.694907 202.74028 \r\nL 245.76494 202.537991 \r\nL 245.834974 202.776749 \r\nL 246.045073 202.758515 \r\nL 246.920486 203.850874 \r\nL 246.955502 203.201841 \r\nL 247.025535 203.585334 \r\nL 247.200618 203.624083 \r\nL 247.305668 204.69251 \r\nL 247.690849 202.869631 \r\nL 247.900948 202.569902 \r\nL 248.251114 201.252461 \r\nL 248.776362 202.567053 \r\nL 249.266593 200.830218 \r\nL 249.616758 201.348762 \r\nL 249.756824 202.504941 \r\nL 249.966923 202.449669 \r\nL 250.387122 202.47588 \r\nL 250.492171 203.654854 \r\nL 250.667254 204.119263 \r\nL 251.122469 203.758562 \r\nL 251.227518 204.089062 \r\nL 251.927849 205.024718 \r\nL 252.943328 203.939198 \r\nL 253.678675 204.489081 \r\nL 254.13389 204.671426 \r\nL 254.869237 203.924952 \r\nL 255.04432 203.181897 \r\nL 255.639601 204.318133 \r\nL 255.814683 203.999029 \r\nL 255.8497 204.06285 \r\nL 256.55003 202.182419 \r\nL 256.760129 202.102074 \r\nL 256.830162 202.445679 \r\nL 257.810625 202.446249 \r\nL 258.335873 201.876421 \r\nL 258.580989 202.064465 \r\nL 258.756071 202.616628 \r\nL 259.316336 201.365286 \r\nL 259.701517 201.408594 \r\nL 260.436864 200.864408 \r\nL 261.452344 200.66155 \r\nL 261.872542 200.126482 \r\nL 262.012608 201.078093 \r\nL 262.257724 201.212573 \r\nL 262.39779 201.536805 \r\nL 262.502839 201.312862 \r\nL 263.09812 202.184129 \r\nL 265.304161 199.564062 \r\nL 265.829409 199.345817 \r\nL 265.864426 199.759513 \r\nL 266.354657 199.533291 \r\nL 266.599773 199.73444 \r\nL 267.019971 199.184557 \r\nL 267.650269 200.201129 \r\nL 268.735781 199.87063 \r\nL 268.840831 199.235271 \r\nL 268.980897 200.65756 \r\nL 269.120963 200.184035 \r\nL 269.506144 199.598252 \r\nL 269.576178 199.838149 \r\nL 269.716244 199.638139 \r\nL 269.786277 199.904249 \r\nL 270.241492 199.664351 \r\nL 270.55664 199.27573 \r\nL 270.976839 199.383997 \r\nL 271.537103 197.363389 \r\nL 272.237433 197.724089 \r\nL 272.27245 197.476213 \r\nL 272.482549 197.969684 \r\nL 273.217896 198.843231 \r\nL 273.673111 197.814121 \r\nL 273.708128 197.858569 \r\nL 273.953243 197.106396 \r\nL 274.163342 196.988441 \r\nL 274.198359 197.072776 \r\nL 274.233375 196.761081 \r\nL 274.863673 197.183892 \r\nL 275.143805 195.939389 \r\nL 275.388921 194.220789 \r\nL 276.229317 193.673185 \r\nL 276.579483 192.969448 \r\nL 277.069714 192.79907 \r\nL 277.20978 191.96997 \r\nL 277.454896 192.473129 \r\nL 277.559945 192.164281 \r\nL 278.085193 192.334661 \r\nL 279.100672 194.987207 \r\nL 279.345788 194.955867 \r\nL 279.660937 194.387179 \r\nL 279.801003 193.669766 \r\nL 280.046119 193.872054 \r\nL 280.116152 193.718771 \r\nL 280.326251 193.62076 \r\nL 281.516813 196.392972 \r\nL 281.586846 197.575365 \r\nL 282.322193 198.067695 \r\nL 283.512755 195.83454 \r\nL 283.722854 196.416904 \r\nL 283.792887 196.329721 \r\nL 283.932953 196.381575 \r\nL 284.073019 195.881837 \r\nL 285.018465 197.053402 \r\nL 285.158531 197.015794 \r\nL 285.403647 196.749115 \r\nL 285.718796 195.754765 \r\nL 285.928895 195.41002 \r\nL 286.033944 195.569001 \r\nL 286.244044 195.150747 \r\nL 286.489159 196.016316 \r\nL 286.734275 196.008908 \r\nL 286.769291 196.39753 \r\nL 287.014407 196.471039 \r\nL 287.18949 196.798689 \r\nL 288.415068 197.991908 \r\nL 288.590151 197.502427 \r\nL 289.080382 195.762742 \r\nL 289.815729 198.075103 \r\nL 289.885762 197.922958 \r\nL 290.375993 197.308115 \r\nL 291.076324 197.205545 \r\nL 291.11134 197.087591 \r\nL 291.601572 193.583151 \r\nL 292.02177 193.495399 \r\nL 292.056787 193.612784 \r\nL 292.12682 193.026431 \r\nL 292.161836 193.227009 \r\nL 292.301902 192.928991 \r\nL 292.792134 193.832736 \r\nL 293.037249 193.695408 \r\nL 293.352398 193.955819 \r\nL 293.492464 193.543265 \r\nL 293.562497 193.789999 \r\nL 293.597514 193.676604 \r\nL 293.63253 193.403656 \r\nL 293.73758 193.553521 \r\nL 294.823092 190.563636 \r\nL 294.858109 190.175584 \r\nL 295.068208 190.255929 \r\nL 295.103224 190.613781 \r\nL 296.013654 190.216611 \r\nL 296.083687 190.74655 \r\nL 296.503885 189.768157 \r\nL 296.784018 189.946514 \r\nL 297.06415 188.928801 \r\nL 297.519365 184.720626 \r\nL 297.974579 184.288125 \r\nL 298.009596 184.718344 \r\nL 298.534844 186.712172 \r\nL 298.67491 188.313388 \r\nL 299.130125 185.397581 \r\nL 299.445273 184.182138 \r\nL 299.725406 185.015226 \r\nL 300.145604 184.261345 \r\nL 300.355703 183.537093 \r\nL 300.986001 179.222928 \r\nL 301.581282 177.974437 \r\nL 302.071513 176.193726 \r\nL 302.141546 176.923675 \r\nL 302.316629 176.292305 \r\nL 302.351645 177.006871 \r\nL 302.596761 176.381198 \r\nL 302.911909 177.120837 \r\nL 303.086992 175.623328 \r\nL 303.577223 181.097662 \r\nL 303.787323 182.307404 \r\nL 303.892372 180.896511 \r\nL 304.067455 181.647547 \r\nL 304.137488 181.439558 \r\nL 304.382604 186.275684 \r\nL 304.767785 185.194723 \r\nL 304.872835 184.408358 \r\nL 305.32805 181.981465 \r\nL 305.538149 182.963276 \r\nL 306.308512 181.061192 \r\nL 306.518612 180.580257 \r\nL 307.043859 178.818922 \r\nL 307.078876 178.932317 \r\nL 307.323992 180.584817 \r\nL 307.464058 175.92078 \r\nL 307.534091 176.658704 \r\nL 308.269438 174.524703 \r\nL 309.004785 174.002739 \r\nL 309.074818 174.202178 \r\nL 309.284917 173.742327 \r\nL 309.424983 175.492839 \r\nL 309.46 174.547494 \r\nL 309.705115 172.857387 \r\nL 310.16033 172.724046 \r\nL 310.440462 172.97249 \r\nL 310.545512 171.649352 \r\nL 310.685578 171.548491 \r\nL 310.720595 171.886969 \r\nL 310.790628 171.240787 \r\nL 311.035743 170.865839 \r\nL 311.175809 171.351333 \r\nL 311.210826 171.317712 \r\nL 311.420925 171.441934 \r\nL 311.701057 171.059582 \r\nL 311.911156 170.186606 \r\nL 312.226305 170.526221 \r\nL 312.961652 166.928332 \r\nL 312.996669 167.055403 \r\nL 313.59195 167.131758 \r\nL 313.872082 170.133612 \r\nL 314.082181 170.160963 \r\nL 314.327297 173.913846 \r\nL 314.432346 171.786679 \r\nL 314.642445 170.43505 \r\nL 315.062644 171.312585 \r\nL 315.622908 170.977526 \r\nL 315.868024 171.28808 \r\nL 315.90304 172.158778 \r\nL 315.938057 171.942244 \r\nL 316.043106 171.742234 \r\nL 316.288222 169.358074 \r\nL 316.91852 167.571096 \r\nL 317.058586 167.458842 \r\nL 317.093602 167.025771 \r\nL 317.128619 168.721007 \r\nL 318.039048 166.277587 \r\nL 318.074065 166.515207 \r\nL 318.564296 166.283858 \r\nL 319.369676 163.895709 \r\nL 319.474726 163.2028 \r\nL 319.964957 163.169749 \r\nL 320.105023 160.85568 \r\nL 320.560238 160.890439 \r\nL 321.260569 158.136461 \r\nL 321.435651 157.734734 \r\nL 321.575717 159.905208 \r\nL 321.820833 158.18091 \r\nL 321.960899 158.327923 \r\nL 322.170998 158.248719 \r\nL 322.241031 158.456704 \r\nL 322.276048 159.277257 \r\nL 322.486147 159.8921 \r\nL 322.696246 157.534724 \r\nL 323.151461 155.63378 \r\nL 323.781758 159.7297 \r\nL 323.921825 158.889205 \r\nL 323.956841 159.149046 \r\nL 324.16694 159.79181 \r\nL 324.236973 160.615211 \r\nL 324.657172 159.740527 \r\nL 324.902287 157.928477 \r\nL 325.602618 160.231148 \r\nL 325.672651 159.874435 \r\nL 326.162882 158.07663 \r\nL 326.197899 158.668682 \r\nL 326.407998 159.449346 \r\nL 326.618097 151.906536 \r\nL 326.653113 151.997707 \r\nL 326.933246 150.547499 \r\nL 327.073312 150.545218 \r\nL 327.178361 150.082518 \r\nL 327.423477 147.30689 \r\nL 327.668593 148.654534 \r\nL 327.878692 148.800406 \r\nL 328.368923 147.784406 \r\nL 328.649055 148.248817 \r\nL 329.139287 140.533921 \r\nL 329.314369 141.191505 \r\nL 329.804601 132.212729 \r\nL 329.874634 133.405951 \r\nL 330.364865 131.593326 \r\nL 330.890113 128.068945 \r\nL 331.135229 123.807773 \r\nL 331.275295 124.389564 \r\nL 331.52041 124.484728 \r\nL 331.555427 124.749699 \r\nL 331.975625 135.565592 \r\nL 332.010642 134.510277 \r\nL 332.220741 133.048668 \r\nL 332.570906 127.847851 \r\nL 332.745989 131.68849 \r\nL 332.991105 124.726905 \r\nL 333.061138 125.306992 \r\nL 334.0416 121.348966 \r\nL 334.076617 119.996766 \r\nL 334.496815 116.658149 \r\nL 334.706914 116.035895 \r\nL 334.95203 118.525475 \r\nL 335.162129 117.232532 \r\nL 335.232162 118.033143 \r\nL 335.897476 110.653306 \r\nL 336.212625 112.48074 \r\nL 336.282658 110.992351 \r\nL 336.632823 106.837743 \r\nL 337.018005 106.314068 \r\nL 337.158071 104.751035 \r\nL 337.193088 101.0312 \r\nL 337.753352 106.163633 \r\nL 338.17355 102.356049 \r\nL 338.663782 114.869459 \r\nL 339.434145 119.98195 \r\nL 339.714277 117.309458 \r\nL 340.099459 129.67301 \r\nL 340.309558 124.774768 \r\nL 340.414608 118.577894 \r\nL 340.554674 119.81841 \r\nL 340.624707 122.056124 \r\nL 340.869823 124.17702 \r\nL 340.939856 128.222795 \r\nL 341.395071 130.666786 \r\nL 341.850285 121.404808 \r\nL 342.095401 118.404101 \r\nL 342.340517 120.585966 \r\nL 342.375533 119.694762 \r\nL 342.550616 119.702733 \r\nL 342.655665 121.817933 \r\nL 343.566095 121.446411 \r\nL 343.636128 118.235998 \r\nL 343.881244 119.69191 \r\nL 344.056326 118.541424 \r\nL 344.336459 111.221993 \r\nL 345.001773 110.277221 \r\nL 345.211872 109.762665 \r\nL 345.73712 105.964764 \r\nL 345.947219 105.424568 \r\nL 346.192335 111.027109 \r\nL 346.297384 106.18985 \r\nL 346.787615 113.740064 \r\nL 346.962698 115.82392 \r\nL 346.997715 114.920175 \r\nL 347.032731 112.089839 \r\nL 347.312863 107.417254 \r\nL 348.013194 104.176074 \r\nL 348.258309 100.296121 \r\nL 348.95864 108.514171 \r\nL 349.168739 113.012964 \r\nL 349.273789 111.887554 \r\nL 349.86907 114.093355 \r\nL 350.149202 112.284152 \r\nL 350.604417 111.034517 \r\nL 350.709466 112.652259 \r\nL 351.129665 115.98632 \r\nL 351.584879 115.941872 \r\nL 351.654912 113.605579 \r\nL 352.075111 113.625528 \r\nL 352.110127 114.44437 \r\nL 352.390259 112.560517 \r\nL 352.915507 116.039887 \r\nL 353.160623 112.273892 \r\nL 353.370722 115.711096 \r\nL 353.405739 115.077448 \r\nL 353.650854 114.545231 \r\nL 353.79092 112.851698 \r\nL 354.036036 112.718361 \r\nL 354.281152 109.593428 \r\nL 354.771383 108.78541 \r\nL 355.541747 98.651029 \r\nL 356.627259 112.284723 \r\nL 357.467656 102.977162 \r\nL 358.273036 78.161174 \r\nL 359.008383 79.957842 \r\nL 359.183465 76.633463 \r\nL 359.428581 76.884182 \r\nL 359.708713 73.662389 \r\nL 360.44406 57.364751 \r\nL 360.794225 40.689882 \r\nL 360.899275 43.933335 \r\nL 361.004325 32.54363 \r\nL 361.774688 34.556265 \r\nL 361.879738 37.564386 \r\nL 362.019804 34.397851 \r\nL 362.510035 27.716053 \r\nL 363.000267 27.151915 \r\nL 363.245382 38.072668 \r\nL 363.630564 43.948152 \r\nL 364.190828 33.082114 \r\nL 364.365911 18.660929 \r\nL 364.68106 33.888428 \r\nL 365.346374 41.944643 \r\nL 365.38139 36.111326 \r\nL 365.451423 36.600229 \r\nL 365.626506 37.930781 \r\nL 366.29182 33.837706 \r\nL 367.062183 27.651655 \r\nL 367.587431 37.999728 \r\nL 367.867564 27.187251 \r\nL 368.357795 23.87371 \r\nL 368.392811 25.538174 \r\nL 368.532878 26.560441 \r\nL 368.883043 33.641118 \r\nL 369.548357 40.6557 \r\nL 370.213671 39.624877 \r\nL 370.248687 40.789602 \r\nL 370.283704 38.579238 \r\nL 370.598853 22.3215 \r\nL 370.703902 21.14708 \r\nL 370.808952 21.178993 \r\nL 371.019051 26.330228 \r\nL 371.054067 24.515907 \r\nL 371.299183 17.083636 \r\nL 371.3342 17.305307 \r\nL 371.579315 27.323434 \r\nL 371.824431 31.23873 \r\nL 371.999514 30.731576 \r\nL 372.069547 32.368119 \r\nL 372.279646 30.827873 \r\nL 372.664828 28.89046 \r\nL 372.909943 22.108385 \r\nL 372.909943 22.108385 \r\n\" style=\"fill:none;stroke:#0000ff;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_16\">\r\n    <path clip-path=\"url(#p7ba2364ce6)\" d=\"M 68.546307 214.692577 \r\nL 68.581323 214.708212 \r\nL 69.701852 214.604439 \r\nL 69.771885 214.494566 \r\nL 69.946968 214.627564 \r\nL 70.192083 214.369074 \r\nL 70.437199 214.653995 \r\nL 71.067497 214.599625 \r\nL 71.207563 214.441353 \r\nL 71.802844 214.550321 \r\nL 72.188025 214.511037 \r\nL 72.258058 214.603579 \r\nL 73.238521 214.493502 \r\nL 73.518653 214.606846 \r\nL 74.429083 214.506262 \r\nL 74.639182 214.551616 \r\nL 75.584628 214.506238 \r\nL 75.829744 214.073063 \r\nL 76.109876 214.092057 \r\nL 76.319975 214.521869 \r\nL 76.460041 214.244904 \r\nL 76.88024 214.296555 \r\nL 77.405488 214.205761 \r\nL 77.650603 214.294607 \r\nL 78.105818 214.63423 \r\nL 78.140835 214.524076 \r\nL 78.420967 214.277419 \r\nL 78.59605 214.410459 \r\nL 78.911198 214.154454 \r\nL 79.261364 214.456754 \r\nL 79.40143 213.596647 \r\nL 80.872124 213.308168 \r\nL 81.852586 213.519179 \r\nL 82.798033 213.436085 \r\nL 83.043148 213.149096 \r\nL 83.673446 213.632193 \r\nL 83.743479 214.015983 \r\nL 83.778495 213.898965 \r\nL 84.303743 213.205087 \r\nL 85.14414 211.745802 \r\nL 85.214173 211.928857 \r\nL 85.424272 213.087943 \r\nL 85.459288 212.72297 \r\nL 85.669388 212.791733 \r\nL 85.739421 212.128993 \r\nL 85.774437 212.294788 \r\nL 85.984536 211.105393 \r\nL 86.64985 210.663429 \r\nL 87.245131 210.444364 \r\nL 87.350181 210.090771 \r\nL 87.630313 211.959931 \r\nL 87.910445 211.853739 \r\nL 87.980478 211.455459 \r\nL 88.36566 211.294278 \r\nL 88.47071 210.914917 \r\nL 88.890908 210.75348 \r\nL 89.17104 211.027685 \r\nL 89.206057 210.754021 \r\nL 89.556222 210.183884 \r\nL 90.326585 209.752674 \r\nL 90.536685 208.836319 \r\nL 90.851833 210.376889 \r\nL 91.166982 211.34967 \r\nL 91.342065 210.884748 \r\nL 91.412098 211.425428 \r\nL 91.622197 211.115449 \r\nL 91.832296 211.57159 \r\nL 91.902329 211.492969 \r\nL 92.042395 211.494415 \r\nL 92.147445 211.880955 \r\nL 92.322527 211.336483 \r\nL 93.583122 211.26119 \r\nL 93.793221 211.536509 \r\nL 93.968304 212.129865 \r\nL 94.073354 211.854098 \r\nL 94.248436 212.358573 \r\nL 94.318469 211.988897 \r\nL 94.738668 211.346694 \r\nL 94.948767 211.527142 \r\nL 95.333949 211.012921 \r\nL 95.754147 211.057794 \r\nL 95.999263 209.955167 \r\nL 96.069296 210.015319 \r\nL 96.209362 210.900589 \r\nL 96.559527 210.431748 \r\nL 96.73461 209.95862 \r\nL 97.53999 210.941204 \r\nL 97.785105 210.681779 \r\nL 98.135271 209.51362 \r\nL 98.625502 208.838642 \r\nL 98.975667 209.562927 \r\nL 99.360849 209.730698 \r\nL 99.711014 211.124105 \r\nL 99.746031 211.073953 \r\nL 99.991146 211.39835 \r\nL 100.936593 210.898176 \r\nL 101.181708 210.535266 \r\nL 101.56689 211.110945 \r\nL 101.952072 211.086596 \r\nL 102.197187 210.834287 \r\nL 102.547353 211.156064 \r\nL 102.652402 210.93526 \r\nL 103.352733 211.565837 \r\nL 103.387749 211.909319 \r\nL 103.527815 211.524537 \r\nL 103.772931 211.645485 \r\nL 104.333195 212.063326 \r\nL 104.543295 211.885391 \r\nL 104.998509 212.262869 \r\nL 105.068543 212.039344 \r\nL 105.838906 212.981574 \r\nL 106.013989 212.972375 \r\nL 106.329137 212.734561 \r\nL 106.50422 212.81904 \r\nL 107.3096 212.690176 \r\nL 107.484683 212.92536 \r\nL 107.799831 212.25908 \r\nL 108.815311 212.495368 \r\nL 109.305542 213.239647 \r\nL 110.215972 212.860086 \r\nL 110.776236 212.982839 \r\nL 111.196434 213.410775 \r\nL 111.231451 213.60848 \r\nL 111.686666 213.591713 \r\nL 113.647591 213.987349 \r\nL 113.682608 213.911035 \r\nL 113.96274 214.061912 \r\nL 114.172839 214.042411 \r\nL 114.347922 213.698483 \r\nL 114.452971 214.266095 \r\nL 114.698087 214.161429 \r\nL 115.188318 214.2911 \r\nL 115.398417 214.17347 \r\nL 115.5735 213.665326 \r\nL 115.818616 214.173895 \r\nL 115.853632 213.936207 \r\nL 116.764062 213.979484 \r\nL 117.359343 213.841936 \r\nL 117.884591 214.041173 \r\nL 118.09469 213.866506 \r\nL 118.129706 213.92821 \r\nL 118.234756 214.053987 \r\nL 118.339806 213.993997 \r\nL 118.514888 214.045073 \r\nL 119.250235 213.85942 \r\nL 119.530367 214.1619 \r\nL 119.950566 214.065764 \r\nL 120.335747 214.230984 \r\nL 121.246177 214.324575 \r\nL 121.806442 214.392379 \r\nL 123.452218 214.40305 \r\nL 123.627301 213.900113 \r\nL 123.697334 213.295156 \r\nL 123.73235 213.474428 \r\nL 124.222582 214.148699 \r\nL 124.74783 214.032028 \r\nL 124.922912 214.194451 \r\nL 124.957929 214.12154 \r\nL 125.168028 214.140945 \r\nL 125.483177 213.909189 \r\nL 125.973408 214.183913 \r\nL 126.393606 214.021897 \r\nL 126.918854 213.916808 \r\nL 127.444102 213.832329 \r\nL 128.354532 213.974496 \r\nL 128.87978 213.342392 \r\nL 129.159912 214.006382 \r\nL 129.370011 213.683466 \r\nL 129.510077 213.748036 \r\nL 130.840705 213.801822 \r\nL 131.471002 213.966838 \r\nL 131.821168 213.877407 \r\nL 132.276383 214.030479 \r\nL 133.536977 213.971415 \r\nL 133.71206 213.703408 \r\nL 133.81711 213.934571 \r\nL 134.202291 213.785303 \r\nL 134.762556 213.754019 \r\nL 134.797572 213.624834 \r\nL 135.217771 213.70375 \r\nL 135.392853 213.561472 \r\nL 135.42787 213.657533 \r\nL 136.1282 213.329266 \r\nL 136.23325 213.445865 \r\nL 137.388795 213.546236 \r\nL 139.45477 213.653612 \r\nL 140.120084 213.529388 \r\nL 140.575299 213.320346 \r\nL 141.835894 213.333346 \r\nL 142.045993 213.078729 \r\nL 142.256092 213.079934 \r\nL 143.096489 212.807889 \r\nL 143.796819 212.777601 \r\nL 144.252034 212.533109 \r\nL 144.602199 212.732214 \r\nL 145.232497 212.642229 \r\nL 145.337546 212.529371 \r\nL 145.442596 212.584157 \r\nL 145.687712 212.505655 \r\nL 145.722728 212.270893 \r\nL 145.967844 212.294668 \r\nL 146.318009 212.078574 \r\nL 146.528108 212.208631 \r\nL 147.01834 211.827008 \r\nL 147.683654 211.654028 \r\nL 147.928769 211.334602 \r\nL 148.138868 211.587657 \r\nL 148.874215 211.999451 \r\nL 149.43448 211.94718 \r\nL 149.959728 211.941892 \r\nL 150.34491 211.944752 \r\nL 152.095736 212.326243 \r\nL 152.165769 212.202915 \r\nL 152.445901 212.399155 \r\nL 153.391347 212.377939 \r\nL 154.021645 212.083247 \r\nL 154.546893 212.057535 \r\nL 155.387289 212.532578 \r\nL 155.597388 212.076955 \r\nL 155.98257 211.965194 \r\nL 156.052603 212.12926 \r\nL 156.297719 211.905462 \r\nL 157.488281 212.047242 \r\nL 157.733396 212.266103 \r\nL 157.838446 212.638737 \r\nL 158.013529 212.769852 \r\nL 158.538776 212.627521 \r\nL 158.713859 212.707881 \r\nL 159.029008 212.583186 \r\nL 160.044487 212.384506 \r\nL 160.779834 212.518763 \r\nL 160.989933 212.914033 \r\nL 161.480164 212.622848 \r\nL 161.620231 212.446565 \r\nL 161.760297 212.648518 \r\nL 162.110462 212.6402 \r\nL 162.180495 212.468889 \r\nL 162.600693 212.577 \r\nL 163.721222 212.071596 \r\nL 163.896305 212.368905 \r\nL 164.561619 212.277835 \r\nL 164.631652 212.955829 \r\nL 164.666668 212.909609 \r\nL 165.121883 212.835549 \r\nL 165.1569 212.741635 \r\nL 165.296966 212.785586 \r\nL 166.277428 212.913569 \r\nL 167.047792 212.872774 \r\nL 167.362941 212.585881 \r\nL 167.888189 212.89003 \r\nL 168.098288 212.713837 \r\nL 168.27337 212.846847 \r\nL 169.113767 212.786896 \r\nL 170.724527 212.512954 \r\nL 171.004659 212.266924 \r\nL 171.529907 212.430247 \r\nL 172.195221 212.281384 \r\nL 172.51037 212.444164 \r\nL 173.770965 212.339467 \r\nL 174.156147 212.541351 \r\nL 174.261196 212.457508 \r\nL 174.646378 211.761685 \r\nL 174.891494 212.202013 \r\nL 175.871956 212.114252 \r\nL 176.117072 212.150152 \r\nL 176.187105 212.032908 \r\nL 177.867898 212.690877 \r\nL 178.638262 212.771816 \r\nL 179.05846 212.822313 \r\nL 179.513675 212.765446 \r\nL 180.003906 212.820502 \r\nL 180.389088 212.816132 \r\nL 180.739253 212.953173 \r\nL 180.879319 212.74546 \r\nL 181.299518 213.017576 \r\nL 181.369551 212.845771 \r\nL 181.999848 212.896822 \r\nL 182.350013 212.611958 \r\nL 183.08536 212.915097 \r\nL 183.260443 212.684516 \r\nL 183.575592 213.370568 \r\nL 184.065823 213.165276 \r\nL 184.310939 213.278772 \r\nL 184.451005 213.0504 \r\nL 185.501501 213.077703 \r\nL 185.7116 212.90389 \r\nL 186.236848 212.958946 \r\nL 186.41193 212.640886 \r\nL 186.446947 212.099235 \r\nL 186.867145 212.72589 \r\nL 186.902162 212.621031 \r\nL 187.672525 212.360114 \r\nL 187.742558 212.479206 \r\nL 188.93312 212.509702 \r\nL 189.388335 212.674184 \r\nL 191.664409 212.628651 \r\nL 191.769459 212.692774 \r\nL 192.119624 212.529041 \r\nL 192.749921 212.31934 \r\nL 192.819954 210.926356 \r\nL 192.995037 210.667008 \r\nL 193.485268 211.323939 \r\nL 193.835434 210.596035 \r\nL 194.220615 211.006922 \r\nL 194.465731 210.735248 \r\nL 194.500748 210.939453 \r\nL 194.570781 210.737051 \r\nL 195.025996 210.888809 \r\nL 195.306128 211.084311 \r\nL 195.516227 210.755395 \r\nL 195.761343 210.877969 \r\nL 195.796359 210.780151 \r\nL 196.041475 208.943083 \r\nL 196.216557 210.565349 \r\nL 196.426657 210.510648 \r\nL 197.547185 210.082022 \r\nL 198.212499 209.613184 \r\nL 198.282532 209.714631 \r\nL 199.122929 209.695184 \r\nL 199.192962 209.266196 \r\nL 199.262995 209.624246 \r\nL 199.683193 209.787729 \r\nL 199.928309 210.174428 \r\nL 200.138408 210.261446 \r\nL 200.733689 209.667908 \r\nL 200.873755 209.65184 \r\nL 201.118871 209.871332 \r\nL 201.223921 209.612365 \r\nL 201.924251 209.899964 \r\nL 202.204383 210.222746 \r\nL 203.04478 210.56695 \r\nL 203.184846 210.814485 \r\nL 204.270358 211.092809 \r\nL 204.76059 210.844625 \r\nL 204.865639 210.583224 \r\nL 205.145771 210.689131 \r\nL 205.390887 210.205134 \r\nL 205.811085 210.580691 \r\nL 205.881118 210.425976 \r\nL 206.336333 210.633802 \r\nL 207.07168 210.047749 \r\nL 207.701978 210.352367 \r\nL 207.807027 210.418642 \r\nL 208.717457 211.108498 \r\nL 208.78749 210.027105 \r\nL 208.962573 210.847745 \r\nL 209.662903 210.243593 \r\nL 210.188151 210.138686 \r\nL 210.293201 210.394319 \r\nL 210.713399 210.574832 \r\nL 210.923498 210.961873 \r\nL 210.993531 210.725757 \r\nL 211.133597 211.261635 \r\nL 211.413729 211.252073 \r\nL 211.693862 211.958537 \r\nL 211.763895 211.416607 \r\nL 211.903961 212.089346 \r\nL 211.973994 211.931899 \r\nL 212.184093 211.606792 \r\nL 212.464225 212.264137 \r\nL 212.744357 211.912053 \r\nL 212.954456 212.53328 \r\nL 213.094523 212.236342 \r\nL 213.584754 211.978904 \r\nL 214.074985 211.949751 \r\nL 214.565217 211.644011 \r\nL 215.090464 211.900615 \r\nL 215.370597 211.929204 \r\nL 216.316043 211.043628 \r\nL 216.421092 211.298287 \r\nL 217.121423 210.909192 \r\nL 217.261489 210.687428 \r\nL 217.331522 210.858925 \r\nL 217.366539 210.74398 \r\nL 217.506605 210.82706 \r\nL 218.802216 210.149578 \r\nL 219.327464 210.594234 \r\nL 219.747662 209.953797 \r\nL 219.852712 210.073298 \r\nL 220.062811 210.067874 \r\nL 220.798158 210.451904 \r\nL 221.463472 209.955541 \r\nL 221.568522 209.839523 \r\nL 221.813637 209.985 \r\nL 221.918687 209.908197 \r\nL 222.023736 210.118288 \r\nL 222.7941 210.190991 \r\nL 222.89915 210.328934 \r\nL 223.179282 210.229162 \r\nL 223.249315 210.069943 \r\nL 223.389381 210.210872 \r\nL 223.49443 209.631376 \r\nL 223.774563 209.882666 \r\nL 224.159744 209.658562 \r\nL 224.229778 209.304843 \r\nL 224.720009 209.522549 \r\nL 225.140207 208.556346 \r\nL 225.385323 207.468663 \r\nL 225.595422 207.384665 \r\nL 226.715951 207.586141 \r\nL 226.821 207.275532 \r\nL 227.101133 207.146763 \r\nL 227.416281 207.418437 \r\nL 228.571827 208.293749 \r\nL 229.132091 208.231505 \r\nL 229.307174 207.584451 \r\nL 229.377207 207.804458 \r\nL 229.587306 207.665925 \r\nL 229.657339 207.349502 \r\nL 230.497736 207.484175 \r\nL 231.268099 206.672878 \r\nL 231.373149 206.988942 \r\nL 232.353611 207.765781 \r\nL 233.334074 208.026033 \r\nL 233.824305 208.308203 \r\nL 233.964371 208.09425 \r\nL 234.314537 208.215667 \r\nL 234.559652 208.39908 \r\nL 234.979851 207.859492 \r\nL 235.540115 207.721776 \r\nL 235.750214 207.135203 \r\nL 235.99533 207.080803 \r\nL 236.69566 205.981486 \r\nL 237.150875 206.064282 \r\nL 237.501041 205.282992 \r\nL 237.746156 205.65177 \r\nL 237.956255 205.07944 \r\nL 238.341437 205.813165 \r\nL 238.866685 205.204787 \r\nL 239.356916 204.937634 \r\nL 240.12728 204.282967 \r\nL 240.197313 204.418321 \r\nL 240.442429 204.301057 \r\nL 240.82761 204.806244 \r\nL 240.897643 204.450615 \r\nL 241.03771 205.255029 \r\nL 241.668007 204.365942 \r\nL 242.298304 205.427839 \r\nL 242.613453 205.680104 \r\nL 242.788536 205.66226 \r\nL 242.823552 205.739295 \r\nL 243.313784 204.54013 \r\nL 243.523883 204.630294 \r\nL 244.25923 203.825391 \r\nL 244.294246 203.900631 \r\nL 244.364279 203.879529 \r\nL 244.994577 203.94623 \r\nL 245.239693 203.728463 \r\nL 245.449792 204.396775 \r\nL 245.694907 204.407217 \r\nL 245.76494 204.282638 \r\nL 245.834974 204.413749 \r\nL 245.97504 204.058564 \r\nL 246.045073 204.231806 \r\nL 246.920486 202.799078 \r\nL 246.955502 202.478243 \r\nL 247.025535 202.749785 \r\nL 247.200618 202.78337 \r\nL 247.305668 202.416736 \r\nL 247.550783 203.37745 \r\nL 247.690849 203.301072 \r\nL 247.900948 203.434485 \r\nL 248.251114 203.980191 \r\nL 248.39118 203.410122 \r\nL 248.531246 203.437054 \r\nL 249.266593 202.502599 \r\nL 249.616758 202.782577 \r\nL 249.651775 203.02003 \r\nL 249.756824 202.073143 \r\nL 249.966923 201.847771 \r\nL 250.387122 201.665718 \r\nL 250.492171 201.922269 \r\nL 250.667254 202.803847 \r\nL 251.122469 202.65234 \r\nL 251.227518 203.625274 \r\nL 251.71775 203.832882 \r\nL 252.663196 205.350724 \r\nL 252.943328 204.763623 \r\nL 253.678675 203.759592 \r\nL 254.098873 204.487198 \r\nL 254.13389 204.42627 \r\nL 254.659138 204.816893 \r\nL 255.04432 204.719781 \r\nL 255.639601 204.090296 \r\nL 255.814683 203.424528 \r\nL 255.8497 203.63365 \r\nL 256.55003 204.014923 \r\nL 256.760129 204.381651 \r\nL 256.830162 203.886673 \r\nL 257.810625 202.742718 \r\nL 258.26584 202.671973 \r\nL 258.335873 202.786127 \r\nL 258.580989 202.614825 \r\nL 258.756071 202.344462 \r\nL 259.316336 202.234108 \r\nL 259.701517 202.604637 \r\nL 260.051683 201.640138 \r\nL 260.436864 201.613207 \r\nL 261.452344 200.939099 \r\nL 261.942575 200.136361 \r\nL 262.012608 200.244384 \r\nL 262.257724 200.304368 \r\nL 262.39779 200.747335 \r\nL 262.502839 200.181319 \r\nL 262.958054 200.919206 \r\nL 263.09812 201.719974 \r\nL 265.304161 200.378208 \r\nL 266.039509 199.589295 \r\nL 266.354657 199.684751 \r\nL 266.599773 199.227456 \r\nL 267.019971 200.180119 \r\nL 267.160037 199.951118 \r\nL 267.405153 200.104776 \r\nL 267.650269 199.872424 \r\nL 268.735781 200.132893 \r\nL 268.840831 200.452913 \r\nL 268.980897 200.38334 \r\nL 269.120963 199.538857 \r\nL 269.506144 199.5723 \r\nL 269.576178 199.423271 \r\nL 269.716244 200.725505 \r\nL 269.786277 200.540572 \r\nL 270.241492 199.905874 \r\nL 270.55664 200.19326 \r\nL 271.046872 199.708792 \r\nL 271.186938 198.949988 \r\nL 271.537103 199.432888 \r\nL 272.237433 198.44178 \r\nL 272.412516 197.504893 \r\nL 272.482549 197.634478 \r\nL 273.217896 198.050268 \r\nL 273.498028 198.709526 \r\nL 273.673111 198.563503 \r\nL 273.708128 198.753913 \r\nL 273.953243 199.193156 \r\nL 274.478491 198.061457 \r\nL 274.863673 197.254797 \r\nL 275.108789 197.157941 \r\nL 275.143805 196.856517 \r\nL 275.388921 197.591612 \r\nL 276.229317 194.502642 \r\nL 276.579483 194.575159 \r\nL 277.069714 193.613846 \r\nL 277.20978 192.715747 \r\nL 277.559945 192.827414 \r\nL 278.085193 192.297963 \r\nL 278.890573 192.477998 \r\nL 279.100672 191.857762 \r\nL 279.345788 192.778329 \r\nL 279.801003 194.680419 \r\nL 280.116152 195.368212 \r\nL 280.326251 194.484664 \r\nL 281.376747 195.572927 \r\nL 281.516813 195.507255 \r\nL 281.586846 196.03409 \r\nL 282.322193 196.343532 \r\nL 282.497275 197.880777 \r\nL 282.707374 197.519058 \r\nL 282.742391 197.979631 \r\nL 283.687837 197.407433 \r\nL 283.722854 197.590301 \r\nL 283.75787 197.508645 \r\nL 283.792887 196.692709 \r\nL 283.932953 196.026402 \r\nL 284.073019 196.602289 \r\nL 285.018465 196.401062 \r\nL 285.158531 194.608427 \r\nL 285.298597 194.966432 \r\nL 285.403647 194.641332 \r\nL 285.928895 197.005701 \r\nL 286.033944 197.300433 \r\nL 286.244044 196.352773 \r\nL 286.489159 196.121569 \r\nL 287.014407 195.101745 \r\nL 287.18949 196.114213 \r\nL 288.415068 197.589062 \r\nL 288.590151 197.496012 \r\nL 288.905299 197.867911 \r\nL 289.080382 197.604345 \r\nL 289.675663 196.349583 \r\nL 289.815729 195.73676 \r\nL 289.885762 196.071846 \r\nL 290.375993 197.263371 \r\nL 291.11134 197.477773 \r\nL 291.601572 198.204672 \r\nL 292.02177 197.584405 \r\nL 292.056787 198.032573 \r\nL 292.12682 197.663984 \r\nL 292.161836 194.826481 \r\nL 292.301902 194.249763 \r\nL 292.792134 193.218382 \r\nL 293.352398 193.049867 \r\nL 293.562497 194.034461 \r\nL 293.597514 193.783074 \r\nL 293.63253 193.797764 \r\nL 293.73758 193.852602 \r\nL 294.823092 193.096617 \r\nL 294.858109 193.493436 \r\nL 295.068208 192.85696 \r\nL 295.103224 189.527615 \r\nL 295.558439 190.460791 \r\nL 296.013654 190.838749 \r\nL 296.083687 190.497943 \r\nL 296.328803 190.624111 \r\nL 296.503885 190.181284 \r\nL 297.519365 189.958591 \r\nL 298.009596 188.326913 \r\nL 298.534844 184.26538 \r\nL 298.67491 184.35927 \r\nL 299.130125 184.525735 \r\nL 299.445273 186.3331 \r\nL 299.725406 186.373154 \r\nL 300.145604 185.45639 \r\nL 300.355703 183.510365 \r\nL 300.986001 184.753475 \r\nL 301.581282 179.549796 \r\nL 302.141546 176.67103 \r\nL 302.316629 178.196974 \r\nL 302.351645 177.783053 \r\nL 302.596761 176.739267 \r\nL 302.80686 176.766002 \r\nL 302.876893 176.026712 \r\nL 302.911909 176.354651 \r\nL 302.946926 176.630793 \r\nL 303.086992 176.653132 \r\nL 303.402141 176.359252 \r\nL 303.577223 177.31805 \r\nL 303.787323 176.725112 \r\nL 303.892372 175.912781 \r\nL 304.137488 179.799951 \r\nL 304.277554 179.808295 \r\nL 304.382604 182.021402 \r\nL 304.872835 180.340028 \r\nL 305.32805 186.913208 \r\nL 306.238479 182.464924 \r\nL 306.308512 182.870467 \r\nL 306.518612 182.045935 \r\nL 307.323992 180.723542 \r\nL 307.464058 180.66161 \r\nL 307.534091 180.042201 \r\nL 309.004785 174.238001 \r\nL 309.074818 174.270412 \r\nL 309.424983 173.280904 \r\nL 309.46 173.309134 \r\nL 309.705115 173.961153 \r\nL 310.16033 175.767183 \r\nL 310.685578 171.860676 \r\nL 310.720595 171.945875 \r\nL 310.790628 172.327278 \r\nL 311.035743 172.048238 \r\nL 311.210826 172.865101 \r\nL 311.420925 171.294768 \r\nL 311.701057 170.755567 \r\nL 311.911156 170.996471 \r\nL 312.226305 171.572792 \r\nL 312.646503 170.270563 \r\nL 312.961652 171.031097 \r\nL 312.996669 170.46962 \r\nL 313.59195 166.611477 \r\nL 313.837065 166.187793 \r\nL 314.082181 167.818267 \r\nL 314.187231 168.479885 \r\nL 314.327297 167.611965 \r\nL 314.432346 168.025935 \r\nL 314.642445 170.515522 \r\nL 315.062644 173.747116 \r\nL 315.342776 170.403279 \r\nL 315.622908 171.369679 \r\nL 315.868024 170.976059 \r\nL 315.90304 171.007503 \r\nL 316.043106 171.600873 \r\nL 316.288222 171.164793 \r\nL 316.673404 171.521888 \r\nL 317.058586 169.33095 \r\nL 317.093602 169.487767 \r\nL 317.128619 168.966314 \r\nL 318.039048 168.305371 \r\nL 318.074065 168.073507 \r\nL 318.564296 165.705328 \r\nL 319.369676 166.028635 \r\nL 319.474726 165.889072 \r\nL 319.964957 163.87876 \r\nL 320.070007 162.6669 \r\nL 320.105023 163.030728 \r\nL 320.700304 162.824785 \r\nL 320.805354 162.318872 \r\nL 320.980436 160.342582 \r\nL 321.435651 159.372755 \r\nL 321.575717 159.436311 \r\nL 321.820833 157.43258 \r\nL 321.960899 157.261788 \r\nL 322.276048 156.167972 \r\nL 322.521164 159.016404 \r\nL 322.696246 157.244426 \r\nL 323.151461 158.885372 \r\nL 323.956841 154.033283 \r\nL 324.236973 157.13005 \r\nL 324.657172 158.534861 \r\nL 324.902287 159.861056 \r\nL 325.602618 158.459855 \r\nL 325.672651 157.497298 \r\nL 326.162882 160.141803 \r\nL 326.197899 159.908683 \r\nL 326.407998 160.00409 \r\nL 326.828196 156.555841 \r\nL 327.178361 158.333418 \r\nL 327.423477 151.507629 \r\nL 327.668593 148.267507 \r\nL 327.878692 148.561666 \r\nL 328.368923 145.986681 \r\nL 328.649055 146.999189 \r\nL 328.824138 146.503824 \r\nL 328.859155 146.813536 \r\nL 329.139287 146.024688 \r\nL 329.314369 146.351358 \r\nL 329.664535 143.75079 \r\nL 330.364865 134.157724 \r\nL 330.890113 133.570889 \r\nL 331.135229 128.776019 \r\nL 331.275295 128.67623 \r\nL 332.010642 121.500778 \r\nL 332.220741 123.578924 \r\nL 332.745989 130.432011 \r\nL 332.816022 128.516675 \r\nL 332.991105 129.231241 \r\nL 333.061138 129.936611 \r\nL 334.076617 120.828629 \r\nL 334.496815 121.140873 \r\nL 334.811964 119.150045 \r\nL 334.95203 117.622884 \r\nL 335.232162 116.67798 \r\nL 335.75741 116.798595 \r\nL 335.897476 115.606208 \r\nL 336.212625 113.857424 \r\nL 336.282658 111.880988 \r\nL 336.632823 111.684407 \r\nL 336.947972 111.9685 \r\nL 337.193088 108.426221 \r\nL 337.753352 107.241283 \r\nL 338.138534 101.815596 \r\nL 338.17355 101.917855 \r\nL 338.663782 105.518863 \r\nL 339.434145 112.122372 \r\nL 339.714277 107.855107 \r\nL 339.819327 108.354172 \r\nL 340.099459 120.177487 \r\nL 340.414608 112.124403 \r\nL 340.554674 114.89626 \r\nL 340.624707 118.710794 \r\nL 340.869823 122.805149 \r\nL 340.939856 122.478959 \r\nL 341.395071 117.030943 \r\nL 341.535137 119.916766 \r\nL 341.850285 123.214156 \r\nL 342.025368 129.62843 \r\nL 342.095401 128.335015 \r\nL 342.655665 120.012667 \r\nL 343.566095 117.803772 \r\nL 343.636128 120.482469 \r\nL 343.881244 118.774308 \r\nL 344.056326 119.047891 \r\nL 344.336459 118.844876 \r\nL 345.211872 112.649281 \r\nL 345.73712 109.91685 \r\nL 345.947219 110.228614 \r\nL 346.192335 108.825942 \r\nL 346.43745 105.726218 \r\nL 346.787615 107.227197 \r\nL 346.997715 109.914826 \r\nL 347.032731 108.441281 \r\nL 347.312863 109.328666 \r\nL 348.013194 106.335221 \r\nL 348.15326 107.059609 \r\nL 348.95864 102.95763 \r\nL 349.028673 101.633184 \r\nL 349.168739 101.537144 \r\nL 349.273789 103.620367 \r\nL 349.518904 104.214304 \r\nL 349.86907 110.695069 \r\nL 350.149202 108.619609 \r\nL 350.709466 111.214209 \r\nL 351.129665 108.804059 \r\nL 351.584879 110.828122 \r\nL 351.654912 112.218377 \r\nL 352.075111 113.188037 \r\nL 352.110127 113.635927 \r\nL 352.390259 112.326187 \r\nL 352.670392 112.925118 \r\nL 353.160623 111.364137 \r\nL 353.405739 113.252671 \r\nL 353.650854 113.177061 \r\nL 353.79092 112.697827 \r\nL 354.036036 110.990841 \r\nL 354.281152 113.842678 \r\nL 355.541747 108.34988 \r\nL 356.627259 95.981806 \r\nL 357.22254 106.797434 \r\nL 357.467656 105.518035 \r\nL 358.238019 102.984584 \r\nL 358.273036 103.676383 \r\nL 359.008383 78.172401 \r\nL 359.183465 76.277154 \r\nL 359.428581 78.014015 \r\nL 359.533631 76.964592 \r\nL 359.708713 72.945903 \r\nL 361.004325 40.522815 \r\nL 361.879738 30.950311 \r\nL 362.019804 30.285661 \r\nL 362.510035 29.629566 \r\nL 363.000267 29.284164 \r\nL 363.245382 29.278919 \r\nL 363.630564 29.533463 \r\nL 363.980729 29.116735 \r\nL 364.190828 29.115372 \r\nL 364.330895 29.310123 \r\nL 364.365911 29.269111 \r\nL 366.29182 29.168223 \r\nL 369.548357 29.205868 \r\nL 370.283704 29.270252 \r\nL 370.808952 29.196491 \r\nL 371.579315 29.344569 \r\nL 371.824431 29.278696 \r\nL 372.069547 29.526257 \r\nL 372.279646 29.178823 \r\nL 372.909943 29.21626 \r\nL 372.909943 29.21626 \r\n\" style=\"fill:none;stroke:#ff0000;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 53.328125 224.64 \r\nL 53.328125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 388.128125 224.64 \r\nL 388.128125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 53.328125 224.64 \r\nL 388.128125 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 53.328125 7.2 \r\nL 388.128125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 60.328125 44.55625 \r\nL 166.842188 44.55625 \r\nQ 168.842188 44.55625 168.842188 42.55625 \r\nL 168.842188 14.2 \r\nQ 168.842188 12.2 166.842188 12.2 \r\nL 60.328125 12.2 \r\nQ 58.328125 12.2 58.328125 14.2 \r\nL 58.328125 42.55625 \r\nQ 58.328125 44.55625 60.328125 44.55625 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_17\">\r\n     <path d=\"M 62.328125 20.298437 \r\nL 82.328125 20.298437 \r\n\" style=\"fill:none;stroke:#0000ff;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_18\"/>\r\n    <g id=\"text_17\">\r\n     <!-- Actual Price -->\r\n     <g transform=\"translate(90.328125 23.798437)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 2188 4044 \r\nL 1331 1722 \r\nL 3047 1722 \r\nL 2188 4044 \r\nz\r\nM 1831 4666 \r\nL 2547 4666 \r\nL 4325 0 \r\nL 3669 0 \r\nL 3244 1197 \r\nL 1141 1197 \r\nL 716 0 \r\nL 50 0 \r\nL 1831 4666 \r\nz\r\n\" id=\"DejaVuSans-41\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 1172 4494 \r\nL 1172 3500 \r\nL 2356 3500 \r\nL 2356 3053 \r\nL 1172 3053 \r\nL 1172 1153 \r\nQ 1172 725 1289 603 \r\nQ 1406 481 1766 481 \r\nL 2356 481 \r\nL 2356 0 \r\nL 1766 0 \r\nQ 1100 0 847 248 \r\nQ 594 497 594 1153 \r\nL 594 3053 \r\nL 172 3053 \r\nL 172 3500 \r\nL 594 3500 \r\nL 594 4494 \r\nL 1172 4494 \r\nz\r\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 544 1381 \r\nL 544 3500 \r\nL 1119 3500 \r\nL 1119 1403 \r\nQ 1119 906 1312 657 \r\nQ 1506 409 1894 409 \r\nQ 2359 409 2629 706 \r\nQ 2900 1003 2900 1516 \r\nL 2900 3500 \r\nL 3475 3500 \r\nL 3475 0 \r\nL 2900 0 \r\nL 2900 538 \r\nQ 2691 219 2414 64 \r\nQ 2138 -91 1772 -91 \r\nQ 1169 -91 856 284 \r\nQ 544 659 544 1381 \r\nz\r\nM 1991 3584 \r\nL 1991 3584 \r\nz\r\n\" id=\"DejaVuSans-75\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 603 4863 \r\nL 1178 4863 \r\nL 1178 0 \r\nL 603 0 \r\nL 603 4863 \r\nz\r\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\r\n       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-41\"/>\r\n      <use x=\"66.658203\" xlink:href=\"#DejaVuSans-63\"/>\r\n      <use x=\"121.638672\" xlink:href=\"#DejaVuSans-74\"/>\r\n      <use x=\"160.847656\" xlink:href=\"#DejaVuSans-75\"/>\r\n      <use x=\"224.226562\" xlink:href=\"#DejaVuSans-61\"/>\r\n      <use x=\"285.505859\" xlink:href=\"#DejaVuSans-6c\"/>\r\n      <use x=\"313.289062\" xlink:href=\"#DejaVuSans-20\"/>\r\n      <use x=\"345.076172\" xlink:href=\"#DejaVuSans-50\"/>\r\n      <use x=\"403.628906\" xlink:href=\"#DejaVuSans-72\"/>\r\n      <use x=\"444.742188\" xlink:href=\"#DejaVuSans-69\"/>\r\n      <use x=\"472.525391\" xlink:href=\"#DejaVuSans-63\"/>\r\n      <use x=\"527.505859\" xlink:href=\"#DejaVuSans-65\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_19\">\r\n     <path d=\"M 62.328125 34.976562 \r\nL 82.328125 34.976562 \r\n\" style=\"fill:none;stroke:#ff0000;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_20\"/>\r\n    <g id=\"text_18\">\r\n     <!-- Predicted Price -->\r\n     <g transform=\"translate(90.328125 38.476562)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 2906 2969 \r\nL 2906 4863 \r\nL 3481 4863 \r\nL 3481 0 \r\nL 2906 0 \r\nL 2906 525 \r\nQ 2725 213 2448 61 \r\nQ 2172 -91 1784 -91 \r\nQ 1150 -91 751 415 \r\nQ 353 922 353 1747 \r\nQ 353 2572 751 3078 \r\nQ 1150 3584 1784 3584 \r\nQ 2172 3584 2448 3432 \r\nQ 2725 3281 2906 2969 \r\nz\r\nM 947 1747 \r\nQ 947 1113 1208 752 \r\nQ 1469 391 1925 391 \r\nQ 2381 391 2643 752 \r\nQ 2906 1113 2906 1747 \r\nQ 2906 2381 2643 2742 \r\nQ 2381 3103 1925 3103 \r\nQ 1469 3103 1208 2742 \r\nQ 947 2381 947 1747 \r\nz\r\n\" id=\"DejaVuSans-64\" transform=\"scale(0.015625)\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-50\"/>\r\n      <use x=\"58.552734\" xlink:href=\"#DejaVuSans-72\"/>\r\n      <use x=\"97.416016\" xlink:href=\"#DejaVuSans-65\"/>\r\n      <use x=\"158.939453\" xlink:href=\"#DejaVuSans-64\"/>\r\n      <use x=\"222.416016\" xlink:href=\"#DejaVuSans-69\"/>\r\n      <use x=\"250.199219\" xlink:href=\"#DejaVuSans-63\"/>\r\n      <use x=\"305.179688\" xlink:href=\"#DejaVuSans-74\"/>\r\n      <use x=\"344.388672\" xlink:href=\"#DejaVuSans-65\"/>\r\n      <use x=\"405.912109\" xlink:href=\"#DejaVuSans-64\"/>\r\n      <use x=\"469.388672\" xlink:href=\"#DejaVuSans-20\"/>\r\n      <use x=\"501.175781\" xlink:href=\"#DejaVuSans-50\"/>\r\n      <use x=\"559.728516\" xlink:href=\"#DejaVuSans-72\"/>\r\n      <use x=\"600.841797\" xlink:href=\"#DejaVuSans-69\"/>\r\n      <use x=\"628.625\" xlink:href=\"#DejaVuSans-63\"/>\r\n      <use x=\"683.605469\" xlink:href=\"#DejaVuSans-65\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p7ba2364ce6\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"53.328125\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "# Dibujar la gráfica\n",
    "plot_graph(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021-06-22 19:47:36,733 INFO sqlalchemy.engine.Engine PRAGMA main.table_info(\"stocks\")\n",
      "2021-06-22 19:47:36,734 INFO sqlalchemy.engine.Engine [raw sql] ()\n",
      "2021-06-22 19:47:36,736 INFO sqlalchemy.engine.Engine PRAGMA temp.table_info(\"stocks\")\n",
      "2021-06-22 19:47:36,736 INFO sqlalchemy.engine.Engine [raw sql] ()\n",
      "2021-06-22 19:47:36,739 INFO sqlalchemy.engine.Engine \n",
      "CREATE TABLE stocks (\n",
      "\t\"index\" DATETIME, \n",
      "\topen FLOAT, \n",
      "\thigh FLOAT, \n",
      "\tlow FLOAT, \n",
      "\tclose FLOAT, \n",
      "\tadjclose FLOAT, \n",
      "\tvolume BIGINT, \n",
      "\tticker TEXT, \n",
      "\tadjclose_15 FLOAT, \n",
      "\ttrue_adjclose_15 FLOAT, \n",
      "\tbuy_profit FLOAT, \n",
      "\tsell_profit FLOAT\n",
      ")\n",
      "\n",
      "\n",
      "2021-06-22 19:47:36,740 INFO sqlalchemy.engine.Engine [no key 0.00148s] ()\n",
      "2021-06-22 19:47:36,763 INFO sqlalchemy.engine.Engine COMMIT\n",
      "2021-06-22 19:47:36,765 INFO sqlalchemy.engine.Engine CREATE INDEX ix_stocks_index ON stocks (\"index\")\n",
      "2021-06-22 19:47:36,766 INFO sqlalchemy.engine.Engine [no key 0.00096s] ()\n",
      "2021-06-22 19:47:36,777 INFO sqlalchemy.engine.Engine COMMIT\n",
      "2021-06-22 19:47:36,783 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2021-06-22 19:47:36,797 INFO sqlalchemy.engine.Engine INSERT INTO stocks (\"index\", open, high, low, close, adjclose, volume, ticker, adjclose_15, true_adjclose_15, buy_profit, sell_profit) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
      "2021-06-22 19:47:36,798 INFO sqlalchemy.engine.Engine [generated in 0.01071s] (('1997-08-06 00:00:00.000000', 2.2083330154418945, 2.3125, 2.1875, 2.25, 2.25, 1243200, 'AMZN', 3.4371025562286377, 2.3177080154418945, 0.06770801544189453, 0.0), ('1997-08-07 00:00:00.000000', 2.25, 2.2604169845581055, 2.125, 2.1770830154418945, 2.1770830154418945, 2034000, 'AMZN', 3.162731170654297, 2.375, 0.19791698455810547, 0.0), ('1997-08-18 00:00:00.000000', 2.0520830154418945, 2.0520830154418945, 1.96875, 2.0416669845581055, 2.0416669845581055, 1784400, 'AMZN', 3.6542959213256836, 3.239583015441894, 1.1979160308837886, 0.0), ('1997-08-19 00:00:00.000000', 2.09375, 2.2083330154418945, 2.0520830154418945, 2.1666669845581055, 2.1666669845581055, 1003200, 'AMZN', 3.7611303329467773, 3.302083015441894, 1.1354160308837886, 0.0), ('1997-08-21 00:00:00.000000', 2.1354169845581055, 2.171875, 2.0729169845581055, 2.1145830154418945, 2.1145830154418945, 624000, 'AMZN', 3.1402175426483154, 3.6875, 1.5729169845581055, 0.0), ('1997-09-05 00:00:00.000000', 2.5833330154418945, 2.6666669845581055, 2.4583330154418945, 2.5, 2.5, 1908000, 'AMZN', 4.57661771774292, 4.1666669845581055, 1.6666669845581055, 0.0), ('1997-09-08 00:00:00.000000', 2.53125, 3.0208330154418945, 2.5, 3.0, 3.0, 5648400, 'AMZN', 4.983861446380615, 4.0416669845581055, 1.0416669845581055, 0.0), ('1997-09-10 00:00:00.000000', 3.3125, 3.328125, 3.125, 3.3020830154418945, 3.3020830154418945, 3866400, 'AMZN', 6.912036895751953, 4.0208330154418945, 0.71875, 0.0)  ... displaying 10 of 1201 total bound parameter sets ...  ('2021-05-17 00:00:00.000000', 3245.929931640625, 3292.75, 3234.590087890625, 3270.389892578125, 3270.389892578125, 3723900, 'AMZN', 3258.3310546875, 3264.110107421875, 0.0, 6.27978515625), ('2021-05-24 00:00:00.000000', 3215.5, 3257.949951171875, 3210.5, 3244.989990234375, 3244.989990234375, 2422800, 'AMZN', 3258.392578125, 3383.1298828125, 138.139892578125, 0.0))\n",
      "2021-06-22 19:47:36,803 INFO sqlalchemy.engine.Engine COMMIT\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "engine = create_engine('sqlite:///stock_info.db', echo=True)\n",
    "sqlite_connection = engine.connect()\n",
    "sqlite_table = \"stocks\"\n",
    "final_df.to_sql(sqlite_table, sqlite_connection, if_exists='fail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'sqlite3' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-95c1a537a2c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msqlite_connection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msqlite3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sqlite3' is not defined"
     ]
    }
   ],
   "source": [
    "sqlite_connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}